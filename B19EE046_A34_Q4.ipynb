{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "B19EE046_A34_Q4.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DNC7FToA9ep1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f442a52c-2a34-4797-e4f3-f8a32a64534b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2D6rSGi87LL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b410891-2902-443b-f299-e6a21acefbdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1igaRi_LRURV8nFC-jyoXND54rA2solUZ/B19EE046_Q4\n"
          ]
        }
      ],
      "source": [
        "cd '/content/drive/MyDrive/B19EE046_Q4'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/salesforce/awd-lstm-lm.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5CW-xnYVqQT",
        "outputId": "ddd96fdf-a750-49fb-f4f3-d6262f3c9033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'awd-lstm-lm'...\n",
            "remote: Enumerating objects: 140, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 140 (delta 0), reused 0 (delta 0), pack-reused 137\u001b[K\n",
            "Receiving objects: 100% (140/140), 58.49 KiB | 2.79 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/quark0/darts.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b61XGpTTVxKP",
        "outputId": "d212b203-8b4f-4299-8a8d-ec71f764353c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'darts'...\n",
            "remote: Enumerating objects: 280, done.\u001b[K\n",
            "remote: Total 280 (delta 0), reused 0 (delta 0), pack-reused 280\u001b[K\n",
            "Receiving objects: 100% (280/280), 4.82 MiB | 4.09 MiB/s, done.\n",
            "Resolving deltas: 100% (154/154), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd awd-lstm-lm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeDYP8fcV4EE",
        "outputId": "957c0219-5e7b-4c0e-ebdb-4dfdf3699869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/B19EE046_Q4/awd-lstm-lm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash getdata.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ8lu0zIV_ZX",
        "outputId": "ef3b97ab-cf02-45e7-d497-33a1e7f0108f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Acquiring datasets ===\n",
            "---\n",
            "- Downloading WikiText-2 (WT2)\n",
            "- Downloading WikiText-103 (WT2)\n",
            "--2022-04-28 13:25:53--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.138.0\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.138.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190229076 (181M) [application/zip]\n",
            "Saving to: ‘wikitext-103-v1.zip’\n",
            "\n",
            "wikitext-103-v1.zip 100%[===================>] 181.42M  13.6MB/s    in 15s     \n",
            "\n",
            "2022-04-28 13:26:09 (12.0 MB/s) - ‘wikitext-103-v1.zip’ saved [190229076/190229076]\n",
            "\n",
            "- Downloading enwik8 (Character)\n",
            "--2022-04-28 13:26:15--  http://mattmahoney.net/dc/enwik8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.24\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.24|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36445475 (35M) [application/zip]\n",
            "Saving to: ‘enwik8.zip’\n",
            "\n",
            "enwik8.zip          100%[===================>]  34.76M   277KB/s    in 2m 10s  \n",
            "\n",
            "2022-04-28 13:28:25 (274 KB/s) - ‘enwik8.zip’ saved [36445475/36445475]\n",
            "\n",
            "Length of enwik8: 100000000\n",
            "train.txt will have 90000000 bytes\n",
            "- Tokenizing...\n",
            "- Writing...\n",
            "valid.txt will have 5000000 bytes\n",
            "- Tokenizing...\n",
            "- Writing...\n",
            "test.txt will have 5000000 bytes\n",
            "- Tokenizing...\n",
            "- Writing...\n",
            "- Downloading Penn Treebank (PTB)\n",
            "- Downloading Penn Treebank (Character)\n",
            "---\n",
            "Happy language modeling :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd rnn "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXm8hGiSDxRh",
        "outputId": "758c07df-1175-42c8-b9de-fed0c48ff25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/B19EE046_Q4/darts/rnn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --data '/content/drive/MyDrive/B19EE046_Q4/awd-lstm-lm/data/penn/' --model_path '/content/drive/MyDrive/B19EE046_Q4/darts/rnn/ptb_model.pt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAUg1k_oFK5N",
        "outputId": "5273aeaa-de56-484a-ed60-fe7fa603140a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([82430, 1])\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'model.RNNModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.container.ParameterList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "Args: Namespace(alpha=0, batch_size=64, beta=0.001, bptt=35, clip=0.25, continue_train=False, cuda=True, data='/content/drive/MyDrive/B19EE046_Q4/awd-lstm-lm/data/penn/', dropout=0.75, dropoute=0.2, dropouth=0.3, dropouti=0.2, emsize=850, epochs=8000, gpu=0, log_interval=200, lr=20, max_seq_len_delta=20, model_path='/content/drive/MyDrive/B19EE046_Q4/darts/rnn/ptb_model.pt', n_experts=1, nhid=850, nhidlast=850, nonmono=5, seed=1267, wdecay=5e-07)\n",
            "Model total parameters: 22960000\n",
            "0 82429\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "35 82429\n",
            "70 82429\n",
            "105 82429\n",
            "140 82429\n",
            "175 82429\n",
            "210 82429\n",
            "245 82429\n",
            "280 82429\n",
            "315 82429\n",
            "350 82429\n",
            "385 82429\n",
            "420 82429\n",
            "455 82429\n",
            "490 82429\n",
            "525 82429\n",
            "560 82429\n",
            "595 82429\n",
            "630 82429\n",
            "665 82429\n",
            "700 82429\n",
            "735 82429\n",
            "770 82429\n",
            "805 82429\n",
            "840 82429\n",
            "875 82429\n",
            "910 82429\n",
            "945 82429\n",
            "980 82429\n",
            "1015 82429\n",
            "1050 82429\n",
            "1085 82429\n",
            "1120 82429\n",
            "1155 82429\n",
            "1190 82429\n",
            "1225 82429\n",
            "1260 82429\n",
            "1295 82429\n",
            "1330 82429\n",
            "1365 82429\n",
            "1400 82429\n",
            "1435 82429\n",
            "1470 82429\n",
            "1505 82429\n",
            "1540 82429\n",
            "1575 82429\n",
            "1610 82429\n",
            "1645 82429\n",
            "1680 82429\n",
            "1715 82429\n",
            "1750 82429\n",
            "1785 82429\n",
            "1820 82429\n",
            "1855 82429\n",
            "1890 82429\n",
            "1925 82429\n",
            "1960 82429\n",
            "1995 82429\n",
            "2030 82429\n",
            "2065 82429\n",
            "2100 82429\n",
            "2135 82429\n",
            "2170 82429\n",
            "2205 82429\n",
            "2240 82429\n",
            "2275 82429\n",
            "2310 82429\n",
            "2345 82429\n",
            "2380 82429\n",
            "2415 82429\n",
            "2450 82429\n",
            "2485 82429\n",
            "2520 82429\n",
            "2555 82429\n",
            "2590 82429\n",
            "2625 82429\n",
            "2660 82429\n",
            "2695 82429\n",
            "2730 82429\n",
            "2765 82429\n",
            "2800 82429\n",
            "2835 82429\n",
            "2870 82429\n",
            "2905 82429\n",
            "2940 82429\n",
            "2975 82429\n",
            "3010 82429\n",
            "3045 82429\n",
            "3080 82429\n",
            "3115 82429\n",
            "3150 82429\n",
            "3185 82429\n",
            "3220 82429\n",
            "3255 82429\n",
            "3290 82429\n",
            "3325 82429\n",
            "3360 82429\n",
            "3395 82429\n",
            "3430 82429\n",
            "3465 82429\n",
            "3500 82429\n",
            "3535 82429\n",
            "3570 82429\n",
            "3605 82429\n",
            "3640 82429\n",
            "3675 82429\n",
            "3710 82429\n",
            "3745 82429\n",
            "3780 82429\n",
            "3815 82429\n",
            "3850 82429\n",
            "3885 82429\n",
            "3920 82429\n",
            "3955 82429\n",
            "3990 82429\n",
            "4025 82429\n",
            "4060 82429\n",
            "4095 82429\n",
            "4130 82429\n",
            "4165 82429\n",
            "4200 82429\n",
            "4235 82429\n",
            "4270 82429\n",
            "4305 82429\n",
            "4340 82429\n",
            "4375 82429\n",
            "4410 82429\n",
            "4445 82429\n",
            "4480 82429\n",
            "4515 82429\n",
            "4550 82429\n",
            "4585 82429\n",
            "4620 82429\n",
            "4655 82429\n",
            "4690 82429\n",
            "4725 82429\n",
            "4760 82429\n",
            "4795 82429\n",
            "4830 82429\n",
            "4865 82429\n",
            "4900 82429\n",
            "4935 82429\n",
            "4970 82429\n",
            "5005 82429\n",
            "5040 82429\n",
            "5075 82429\n",
            "5110 82429\n",
            "5145 82429\n",
            "5180 82429\n",
            "5215 82429\n",
            "5250 82429\n",
            "5285 82429\n",
            "5320 82429\n",
            "5355 82429\n",
            "5390 82429\n",
            "5425 82429\n",
            "5460 82429\n",
            "5495 82429\n",
            "5530 82429\n",
            "5565 82429\n",
            "5600 82429\n",
            "5635 82429\n",
            "5670 82429\n",
            "5705 82429\n",
            "5740 82429\n",
            "5775 82429\n",
            "5810 82429\n",
            "5845 82429\n",
            "5880 82429\n",
            "5915 82429\n",
            "5950 82429\n",
            "5985 82429\n",
            "6020 82429\n",
            "6055 82429\n",
            "6090 82429\n",
            "6125 82429\n",
            "6160 82429\n",
            "6195 82429\n",
            "6230 82429\n",
            "6265 82429\n",
            "6300 82429\n",
            "6335 82429\n",
            "6370 82429\n",
            "6405 82429\n",
            "6440 82429\n",
            "6475 82429\n",
            "6510 82429\n",
            "6545 82429\n",
            "6580 82429\n",
            "6615 82429\n",
            "6650 82429\n",
            "6685 82429\n",
            "6720 82429\n",
            "6755 82429\n",
            "6790 82429\n",
            "6825 82429\n",
            "6860 82429\n",
            "6895 82429\n",
            "6930 82429\n",
            "6965 82429\n",
            "7000 82429\n",
            "7035 82429\n",
            "7070 82429\n",
            "7105 82429\n",
            "7140 82429\n",
            "7175 82429\n",
            "7210 82429\n",
            "7245 82429\n",
            "7280 82429\n",
            "7315 82429\n",
            "7350 82429\n",
            "7385 82429\n",
            "7420 82429\n",
            "7455 82429\n",
            "7490 82429\n",
            "7525 82429\n",
            "7560 82429\n",
            "7595 82429\n",
            "7630 82429\n",
            "7665 82429\n",
            "7700 82429\n",
            "7735 82429\n",
            "7770 82429\n",
            "7805 82429\n",
            "7840 82429\n",
            "7875 82429\n",
            "7910 82429\n",
            "7945 82429\n",
            "7980 82429\n",
            "8015 82429\n",
            "8050 82429\n",
            "8085 82429\n",
            "8120 82429\n",
            "8155 82429\n",
            "8190 82429\n",
            "8225 82429\n",
            "8260 82429\n",
            "8295 82429\n",
            "8330 82429\n",
            "8365 82429\n",
            "8400 82429\n",
            "8435 82429\n",
            "8470 82429\n",
            "8505 82429\n",
            "8540 82429\n",
            "8575 82429\n",
            "8610 82429\n",
            "8645 82429\n",
            "8680 82429\n",
            "8715 82429\n",
            "8750 82429\n",
            "8785 82429\n",
            "8820 82429\n",
            "8855 82429\n",
            "8890 82429\n",
            "8925 82429\n",
            "8960 82429\n",
            "8995 82429\n",
            "9030 82429\n",
            "9065 82429\n",
            "9100 82429\n",
            "9135 82429\n",
            "9170 82429\n",
            "9205 82429\n",
            "9240 82429\n",
            "9275 82429\n",
            "9310 82429\n",
            "9345 82429\n",
            "9380 82429\n",
            "9415 82429\n",
            "9450 82429\n",
            "9485 82429\n",
            "9520 82429\n",
            "9555 82429\n",
            "9590 82429\n",
            "9625 82429\n",
            "9660 82429\n",
            "9695 82429\n",
            "9730 82429\n",
            "9765 82429\n",
            "9800 82429\n",
            "9835 82429\n",
            "9870 82429\n",
            "9905 82429\n",
            "9940 82429\n",
            "9975 82429\n",
            "10010 82429\n",
            "10045 82429\n",
            "10080 82429\n",
            "10115 82429\n",
            "10150 82429\n",
            "10185 82429\n",
            "10220 82429\n",
            "10255 82429\n",
            "10290 82429\n",
            "10325 82429\n",
            "10360 82429\n",
            "10395 82429\n",
            "10430 82429\n",
            "10465 82429\n",
            "10500 82429\n",
            "10535 82429\n",
            "10570 82429\n",
            "10605 82429\n",
            "10640 82429\n",
            "10675 82429\n",
            "10710 82429\n",
            "10745 82429\n",
            "10780 82429\n",
            "10815 82429\n",
            "10850 82429\n",
            "10885 82429\n",
            "10920 82429\n",
            "10955 82429\n",
            "10990 82429\n",
            "11025 82429\n",
            "11060 82429\n",
            "11095 82429\n",
            "11130 82429\n",
            "11165 82429\n",
            "11200 82429\n",
            "11235 82429\n",
            "11270 82429\n",
            "11305 82429\n",
            "11340 82429\n",
            "11375 82429\n",
            "11410 82429\n",
            "11445 82429\n",
            "11480 82429\n",
            "11515 82429\n",
            "11550 82429\n",
            "11585 82429\n",
            "11620 82429\n",
            "11655 82429\n",
            "11690 82429\n",
            "11725 82429\n",
            "11760 82429\n",
            "11795 82429\n",
            "11830 82429\n",
            "11865 82429\n",
            "11900 82429\n",
            "11935 82429\n",
            "11970 82429\n",
            "12005 82429\n",
            "12040 82429\n",
            "12075 82429\n",
            "12110 82429\n",
            "12145 82429\n",
            "12180 82429\n",
            "12215 82429\n",
            "12250 82429\n",
            "12285 82429\n",
            "12320 82429\n",
            "12355 82429\n",
            "12390 82429\n",
            "12425 82429\n",
            "12460 82429\n",
            "12495 82429\n",
            "12530 82429\n",
            "12565 82429\n",
            "12600 82429\n",
            "12635 82429\n",
            "12670 82429\n",
            "12705 82429\n",
            "12740 82429\n",
            "12775 82429\n",
            "12810 82429\n",
            "12845 82429\n",
            "12880 82429\n",
            "12915 82429\n",
            "12950 82429\n",
            "12985 82429\n",
            "13020 82429\n",
            "13055 82429\n",
            "13090 82429\n",
            "13125 82429\n",
            "13160 82429\n",
            "13195 82429\n",
            "13230 82429\n",
            "13265 82429\n",
            "13300 82429\n",
            "13335 82429\n",
            "13370 82429\n",
            "13405 82429\n",
            "13440 82429\n",
            "13475 82429\n",
            "13510 82429\n",
            "13545 82429\n",
            "13580 82429\n",
            "13615 82429\n",
            "13650 82429\n",
            "13685 82429\n",
            "13720 82429\n",
            "13755 82429\n",
            "13790 82429\n",
            "13825 82429\n",
            "13860 82429\n",
            "13895 82429\n",
            "13930 82429\n",
            "13965 82429\n",
            "14000 82429\n",
            "14035 82429\n",
            "14070 82429\n",
            "14105 82429\n",
            "14140 82429\n",
            "14175 82429\n",
            "14210 82429\n",
            "14245 82429\n",
            "14280 82429\n",
            "14315 82429\n",
            "14350 82429\n",
            "14385 82429\n",
            "14420 82429\n",
            "14455 82429\n",
            "14490 82429\n",
            "14525 82429\n",
            "14560 82429\n",
            "14595 82429\n",
            "14630 82429\n",
            "14665 82429\n",
            "14700 82429\n",
            "14735 82429\n",
            "14770 82429\n",
            "14805 82429\n",
            "14840 82429\n",
            "14875 82429\n",
            "14910 82429\n",
            "14945 82429\n",
            "14980 82429\n",
            "15015 82429\n",
            "15050 82429\n",
            "15085 82429\n",
            "15120 82429\n",
            "15155 82429\n",
            "15190 82429\n",
            "15225 82429\n",
            "15260 82429\n",
            "15295 82429\n",
            "15330 82429\n",
            "15365 82429\n",
            "15400 82429\n",
            "15435 82429\n",
            "15470 82429\n",
            "15505 82429\n",
            "15540 82429\n",
            "15575 82429\n",
            "15610 82429\n",
            "15645 82429\n",
            "15680 82429\n",
            "15715 82429\n",
            "15750 82429\n",
            "15785 82429\n",
            "15820 82429\n",
            "15855 82429\n",
            "15890 82429\n",
            "15925 82429\n",
            "15960 82429\n",
            "15995 82429\n",
            "16030 82429\n",
            "16065 82429\n",
            "16100 82429\n",
            "16135 82429\n",
            "16170 82429\n",
            "16205 82429\n",
            "16240 82429\n",
            "16275 82429\n",
            "16310 82429\n",
            "16345 82429\n",
            "16380 82429\n",
            "16415 82429\n",
            "16450 82429\n",
            "16485 82429\n",
            "16520 82429\n",
            "16555 82429\n",
            "16590 82429\n",
            "16625 82429\n",
            "16660 82429\n",
            "16695 82429\n",
            "16730 82429\n",
            "16765 82429\n",
            "16800 82429\n",
            "16835 82429\n",
            "16870 82429\n",
            "16905 82429\n",
            "16940 82429\n",
            "16975 82429\n",
            "17010 82429\n",
            "17045 82429\n",
            "17080 82429\n",
            "17115 82429\n",
            "17150 82429\n",
            "17185 82429\n",
            "17220 82429\n",
            "17255 82429\n",
            "17290 82429\n",
            "17325 82429\n",
            "17360 82429\n",
            "17395 82429\n",
            "17430 82429\n",
            "17465 82429\n",
            "17500 82429\n",
            "17535 82429\n",
            "17570 82429\n",
            "17605 82429\n",
            "17640 82429\n",
            "17675 82429\n",
            "17710 82429\n",
            "17745 82429\n",
            "17780 82429\n",
            "17815 82429\n",
            "17850 82429\n",
            "17885 82429\n",
            "17920 82429\n",
            "17955 82429\n",
            "17990 82429\n",
            "18025 82429\n",
            "18060 82429\n",
            "18095 82429\n",
            "18130 82429\n",
            "18165 82429\n",
            "18200 82429\n",
            "18235 82429\n",
            "18270 82429\n",
            "18305 82429\n",
            "18340 82429\n",
            "18375 82429\n",
            "18410 82429\n",
            "18445 82429\n",
            "18480 82429\n",
            "18515 82429\n",
            "18550 82429\n",
            "18585 82429\n",
            "18620 82429\n",
            "18655 82429\n",
            "18690 82429\n",
            "18725 82429\n",
            "18760 82429\n",
            "18795 82429\n",
            "18830 82429\n",
            "18865 82429\n",
            "18900 82429\n",
            "18935 82429\n",
            "18970 82429\n",
            "19005 82429\n",
            "19040 82429\n",
            "19075 82429\n",
            "19110 82429\n",
            "19145 82429\n",
            "19180 82429\n",
            "19215 82429\n",
            "19250 82429\n",
            "19285 82429\n",
            "19320 82429\n",
            "19355 82429\n",
            "19390 82429\n",
            "19425 82429\n",
            "19460 82429\n",
            "19495 82429\n",
            "19530 82429\n",
            "19565 82429\n",
            "19600 82429\n",
            "19635 82429\n",
            "19670 82429\n",
            "19705 82429\n",
            "19740 82429\n",
            "19775 82429\n",
            "19810 82429\n",
            "19845 82429\n",
            "19880 82429\n",
            "19915 82429\n",
            "19950 82429\n",
            "19985 82429\n",
            "20020 82429\n",
            "20055 82429\n",
            "20090 82429\n",
            "20125 82429\n",
            "20160 82429\n",
            "20195 82429\n",
            "20230 82429\n",
            "20265 82429\n",
            "20300 82429\n",
            "20335 82429\n",
            "20370 82429\n",
            "20405 82429\n",
            "20440 82429\n",
            "20475 82429\n",
            "20510 82429\n",
            "20545 82429\n",
            "20580 82429\n",
            "20615 82429\n",
            "20650 82429\n",
            "20685 82429\n",
            "20720 82429\n",
            "20755 82429\n",
            "20790 82429\n",
            "20825 82429\n",
            "20860 82429\n",
            "20895 82429\n",
            "20930 82429\n",
            "20965 82429\n",
            "21000 82429\n",
            "21035 82429\n",
            "21070 82429\n",
            "21105 82429\n",
            "21140 82429\n",
            "21175 82429\n",
            "21210 82429\n",
            "21245 82429\n",
            "21280 82429\n",
            "21315 82429\n",
            "21350 82429\n",
            "21385 82429\n",
            "21420 82429\n",
            "21455 82429\n",
            "21490 82429\n",
            "21525 82429\n",
            "21560 82429\n",
            "21595 82429\n",
            "21630 82429\n",
            "21665 82429\n",
            "21700 82429\n",
            "21735 82429\n",
            "21770 82429\n",
            "21805 82429\n",
            "21840 82429\n",
            "21875 82429\n",
            "21910 82429\n",
            "21945 82429\n",
            "21980 82429\n",
            "22015 82429\n",
            "22050 82429\n",
            "22085 82429\n",
            "22120 82429\n",
            "22155 82429\n",
            "22190 82429\n",
            "22225 82429\n",
            "22260 82429\n",
            "22295 82429\n",
            "22330 82429\n",
            "22365 82429\n",
            "22400 82429\n",
            "22435 82429\n",
            "22470 82429\n",
            "22505 82429\n",
            "22540 82429\n",
            "22575 82429\n",
            "22610 82429\n",
            "22645 82429\n",
            "22680 82429\n",
            "22715 82429\n",
            "22750 82429\n",
            "22785 82429\n",
            "22820 82429\n",
            "22855 82429\n",
            "22890 82429\n",
            "22925 82429\n",
            "22960 82429\n",
            "22995 82429\n",
            "23030 82429\n",
            "23065 82429\n",
            "23100 82429\n",
            "23135 82429\n",
            "23170 82429\n",
            "23205 82429\n",
            "23240 82429\n",
            "23275 82429\n",
            "23310 82429\n",
            "23345 82429\n",
            "23380 82429\n",
            "23415 82429\n",
            "23450 82429\n",
            "23485 82429\n",
            "23520 82429\n",
            "23555 82429\n",
            "23590 82429\n",
            "23625 82429\n",
            "23660 82429\n",
            "23695 82429\n",
            "23730 82429\n",
            "23765 82429\n",
            "23800 82429\n",
            "23835 82429\n",
            "23870 82429\n",
            "23905 82429\n",
            "23940 82429\n",
            "23975 82429\n",
            "24010 82429\n",
            "24045 82429\n",
            "24080 82429\n",
            "24115 82429\n",
            "24150 82429\n",
            "24185 82429\n",
            "24220 82429\n",
            "24255 82429\n",
            "24290 82429\n",
            "24325 82429\n",
            "24360 82429\n",
            "24395 82429\n",
            "24430 82429\n",
            "24465 82429\n",
            "24500 82429\n",
            "24535 82429\n",
            "24570 82429\n",
            "24605 82429\n",
            "24640 82429\n",
            "24675 82429\n",
            "24710 82429\n",
            "24745 82429\n",
            "24780 82429\n",
            "24815 82429\n",
            "24850 82429\n",
            "24885 82429\n",
            "24920 82429\n",
            "24955 82429\n",
            "24990 82429\n",
            "25025 82429\n",
            "25060 82429\n",
            "25095 82429\n",
            "25130 82429\n",
            "25165 82429\n",
            "25200 82429\n",
            "25235 82429\n",
            "25270 82429\n",
            "25305 82429\n",
            "25340 82429\n",
            "25375 82429\n",
            "25410 82429\n",
            "25445 82429\n",
            "25480 82429\n",
            "25515 82429\n",
            "25550 82429\n",
            "25585 82429\n",
            "25620 82429\n",
            "25655 82429\n",
            "25690 82429\n",
            "25725 82429\n",
            "25760 82429\n",
            "25795 82429\n",
            "25830 82429\n",
            "25865 82429\n",
            "25900 82429\n",
            "25935 82429\n",
            "25970 82429\n",
            "26005 82429\n",
            "26040 82429\n",
            "26075 82429\n",
            "26110 82429\n",
            "26145 82429\n",
            "26180 82429\n",
            "26215 82429\n",
            "26250 82429\n",
            "26285 82429\n",
            "26320 82429\n",
            "26355 82429\n",
            "26390 82429\n",
            "26425 82429\n",
            "26460 82429\n",
            "26495 82429\n",
            "26530 82429\n",
            "26565 82429\n",
            "26600 82429\n",
            "26635 82429\n",
            "26670 82429\n",
            "26705 82429\n",
            "26740 82429\n",
            "26775 82429\n",
            "26810 82429\n",
            "26845 82429\n",
            "26880 82429\n",
            "26915 82429\n",
            "26950 82429\n",
            "26985 82429\n",
            "27020 82429\n",
            "27055 82429\n",
            "27090 82429\n",
            "27125 82429\n",
            "27160 82429\n",
            "27195 82429\n",
            "27230 82429\n",
            "27265 82429\n",
            "27300 82429\n",
            "27335 82429\n",
            "27370 82429\n",
            "27405 82429\n",
            "27440 82429\n",
            "27475 82429\n",
            "27510 82429\n",
            "27545 82429\n",
            "27580 82429\n",
            "27615 82429\n",
            "27650 82429\n",
            "27685 82429\n",
            "27720 82429\n",
            "27755 82429\n",
            "27790 82429\n",
            "27825 82429\n",
            "27860 82429\n",
            "27895 82429\n",
            "27930 82429\n",
            "27965 82429\n",
            "28000 82429\n",
            "28035 82429\n",
            "28070 82429\n",
            "28105 82429\n",
            "28140 82429\n",
            "28175 82429\n",
            "28210 82429\n",
            "28245 82429\n",
            "28280 82429\n",
            "28315 82429\n",
            "28350 82429\n",
            "28385 82429\n",
            "28420 82429\n",
            "28455 82429\n",
            "28490 82429\n",
            "28525 82429\n",
            "28560 82429\n",
            "28595 82429\n",
            "28630 82429\n",
            "28665 82429\n",
            "28700 82429\n",
            "28735 82429\n",
            "28770 82429\n",
            "28805 82429\n",
            "28840 82429\n",
            "28875 82429\n",
            "28910 82429\n",
            "28945 82429\n",
            "28980 82429\n",
            "29015 82429\n",
            "29050 82429\n",
            "29085 82429\n",
            "29120 82429\n",
            "29155 82429\n",
            "29190 82429\n",
            "29225 82429\n",
            "29260 82429\n",
            "29295 82429\n",
            "29330 82429\n",
            "29365 82429\n",
            "29400 82429\n",
            "29435 82429\n",
            "29470 82429\n",
            "29505 82429\n",
            "29540 82429\n",
            "29575 82429\n",
            "29610 82429\n",
            "29645 82429\n",
            "29680 82429\n",
            "29715 82429\n",
            "29750 82429\n",
            "29785 82429\n",
            "29820 82429\n",
            "29855 82429\n",
            "29890 82429\n",
            "29925 82429\n",
            "29960 82429\n",
            "29995 82429\n",
            "30030 82429\n",
            "30065 82429\n",
            "30100 82429\n",
            "30135 82429\n",
            "30170 82429\n",
            "30205 82429\n",
            "30240 82429\n",
            "30275 82429\n",
            "30310 82429\n",
            "30345 82429\n",
            "30380 82429\n",
            "30415 82429\n",
            "30450 82429\n",
            "30485 82429\n",
            "30520 82429\n",
            "30555 82429\n",
            "30590 82429\n",
            "30625 82429\n",
            "30660 82429\n",
            "30695 82429\n",
            "30730 82429\n",
            "30765 82429\n",
            "30800 82429\n",
            "30835 82429\n",
            "30870 82429\n",
            "30905 82429\n",
            "30940 82429\n",
            "30975 82429\n",
            "31010 82429\n",
            "31045 82429\n",
            "31080 82429\n",
            "31115 82429\n",
            "31150 82429\n",
            "31185 82429\n",
            "31220 82429\n",
            "31255 82429\n",
            "31290 82429\n",
            "31325 82429\n",
            "31360 82429\n",
            "31395 82429\n",
            "31430 82429\n",
            "31465 82429\n",
            "31500 82429\n",
            "31535 82429\n",
            "31570 82429\n",
            "31605 82429\n",
            "31640 82429\n",
            "31675 82429\n",
            "31710 82429\n",
            "31745 82429\n",
            "31780 82429\n",
            "31815 82429\n",
            "31850 82429\n",
            "31885 82429\n",
            "31920 82429\n",
            "31955 82429\n",
            "31990 82429\n",
            "32025 82429\n",
            "32060 82429\n",
            "32095 82429\n",
            "32130 82429\n",
            "32165 82429\n",
            "32200 82429\n",
            "32235 82429\n",
            "32270 82429\n",
            "32305 82429\n",
            "32340 82429\n",
            "32375 82429\n",
            "32410 82429\n",
            "32445 82429\n",
            "32480 82429\n",
            "32515 82429\n",
            "32550 82429\n",
            "32585 82429\n",
            "32620 82429\n",
            "32655 82429\n",
            "32690 82429\n",
            "32725 82429\n",
            "32760 82429\n",
            "32795 82429\n",
            "32830 82429\n",
            "32865 82429\n",
            "32900 82429\n",
            "32935 82429\n",
            "32970 82429\n",
            "33005 82429\n",
            "33040 82429\n",
            "33075 82429\n",
            "33110 82429\n",
            "33145 82429\n",
            "33180 82429\n",
            "33215 82429\n",
            "33250 82429\n",
            "33285 82429\n",
            "33320 82429\n",
            "33355 82429\n",
            "33390 82429\n",
            "33425 82429\n",
            "33460 82429\n",
            "33495 82429\n",
            "33530 82429\n",
            "33565 82429\n",
            "33600 82429\n",
            "33635 82429\n",
            "33670 82429\n",
            "33705 82429\n",
            "33740 82429\n",
            "33775 82429\n",
            "33810 82429\n",
            "33845 82429\n",
            "33880 82429\n",
            "33915 82429\n",
            "33950 82429\n",
            "33985 82429\n",
            "34020 82429\n",
            "34055 82429\n",
            "34090 82429\n",
            "34125 82429\n",
            "34160 82429\n",
            "34195 82429\n",
            "34230 82429\n",
            "34265 82429\n",
            "34300 82429\n",
            "34335 82429\n",
            "34370 82429\n",
            "34405 82429\n",
            "34440 82429\n",
            "34475 82429\n",
            "34510 82429\n",
            "34545 82429\n",
            "34580 82429\n",
            "34615 82429\n",
            "34650 82429\n",
            "34685 82429\n",
            "34720 82429\n",
            "34755 82429\n",
            "34790 82429\n",
            "34825 82429\n",
            "34860 82429\n",
            "34895 82429\n",
            "34930 82429\n",
            "34965 82429\n",
            "35000 82429\n",
            "35035 82429\n",
            "35070 82429\n",
            "35105 82429\n",
            "35140 82429\n",
            "35175 82429\n",
            "35210 82429\n",
            "35245 82429\n",
            "35280 82429\n",
            "35315 82429\n",
            "35350 82429\n",
            "35385 82429\n",
            "35420 82429\n",
            "35455 82429\n",
            "35490 82429\n",
            "35525 82429\n",
            "35560 82429\n",
            "35595 82429\n",
            "35630 82429\n",
            "35665 82429\n",
            "35700 82429\n",
            "35735 82429\n",
            "35770 82429\n",
            "35805 82429\n",
            "35840 82429\n",
            "35875 82429\n",
            "35910 82429\n",
            "35945 82429\n",
            "35980 82429\n",
            "36015 82429\n",
            "36050 82429\n",
            "36085 82429\n",
            "36120 82429\n",
            "36155 82429\n",
            "36190 82429\n",
            "36225 82429\n",
            "36260 82429\n",
            "36295 82429\n",
            "36330 82429\n",
            "36365 82429\n",
            "36400 82429\n",
            "36435 82429\n",
            "36470 82429\n",
            "36505 82429\n",
            "36540 82429\n",
            "36575 82429\n",
            "36610 82429\n",
            "36645 82429\n",
            "36680 82429\n",
            "36715 82429\n",
            "36750 82429\n",
            "36785 82429\n",
            "36820 82429\n",
            "36855 82429\n",
            "36890 82429\n",
            "36925 82429\n",
            "36960 82429\n",
            "36995 82429\n",
            "37030 82429\n",
            "37065 82429\n",
            "37100 82429\n",
            "37135 82429\n",
            "37170 82429\n",
            "37205 82429\n",
            "37240 82429\n",
            "37275 82429\n",
            "37310 82429\n",
            "37345 82429\n",
            "37380 82429\n",
            "37415 82429\n",
            "37450 82429\n",
            "37485 82429\n",
            "37520 82429\n",
            "37555 82429\n",
            "37590 82429\n",
            "37625 82429\n",
            "37660 82429\n",
            "37695 82429\n",
            "37730 82429\n",
            "37765 82429\n",
            "37800 82429\n",
            "37835 82429\n",
            "37870 82429\n",
            "37905 82429\n",
            "37940 82429\n",
            "37975 82429\n",
            "38010 82429\n",
            "38045 82429\n",
            "38080 82429\n",
            "38115 82429\n",
            "38150 82429\n",
            "38185 82429\n",
            "38220 82429\n",
            "38255 82429\n",
            "38290 82429\n",
            "38325 82429\n",
            "38360 82429\n",
            "38395 82429\n",
            "38430 82429\n",
            "38465 82429\n",
            "38500 82429\n",
            "38535 82429\n",
            "38570 82429\n",
            "38605 82429\n",
            "38640 82429\n",
            "38675 82429\n",
            "38710 82429\n",
            "38745 82429\n",
            "38780 82429\n",
            "38815 82429\n",
            "38850 82429\n",
            "38885 82429\n",
            "38920 82429\n",
            "38955 82429\n",
            "38990 82429\n",
            "39025 82429\n",
            "39060 82429\n",
            "39095 82429\n",
            "39130 82429\n",
            "39165 82429\n",
            "39200 82429\n",
            "39235 82429\n",
            "39270 82429\n",
            "39305 82429\n",
            "39340 82429\n",
            "39375 82429\n",
            "39410 82429\n",
            "39445 82429\n",
            "39480 82429\n",
            "39515 82429\n",
            "39550 82429\n",
            "39585 82429\n",
            "39620 82429\n",
            "39655 82429\n",
            "39690 82429\n",
            "39725 82429\n",
            "39760 82429\n",
            "39795 82429\n",
            "39830 82429\n",
            "39865 82429\n",
            "39900 82429\n",
            "39935 82429\n",
            "39970 82429\n",
            "40005 82429\n",
            "40040 82429\n",
            "40075 82429\n",
            "40110 82429\n",
            "40145 82429\n",
            "40180 82429\n",
            "40215 82429\n",
            "40250 82429\n",
            "40285 82429\n",
            "40320 82429\n",
            "40355 82429\n",
            "40390 82429\n",
            "40425 82429\n",
            "40460 82429\n",
            "40495 82429\n",
            "40530 82429\n",
            "40565 82429\n",
            "40600 82429\n",
            "40635 82429\n",
            "40670 82429\n",
            "40705 82429\n",
            "40740 82429\n",
            "40775 82429\n",
            "40810 82429\n",
            "40845 82429\n",
            "40880 82429\n",
            "40915 82429\n",
            "40950 82429\n",
            "40985 82429\n",
            "41020 82429\n",
            "41055 82429\n",
            "41090 82429\n",
            "41125 82429\n",
            "41160 82429\n",
            "41195 82429\n",
            "41230 82429\n",
            "41265 82429\n",
            "41300 82429\n",
            "41335 82429\n",
            "41370 82429\n",
            "41405 82429\n",
            "41440 82429\n",
            "41475 82429\n",
            "41510 82429\n",
            "41545 82429\n",
            "41580 82429\n",
            "41615 82429\n",
            "41650 82429\n",
            "41685 82429\n",
            "41720 82429\n",
            "41755 82429\n",
            "41790 82429\n",
            "41825 82429\n",
            "41860 82429\n",
            "41895 82429\n",
            "41930 82429\n",
            "41965 82429\n",
            "42000 82429\n",
            "42035 82429\n",
            "42070 82429\n",
            "42105 82429\n",
            "42140 82429\n",
            "42175 82429\n",
            "42210 82429\n",
            "42245 82429\n",
            "42280 82429\n",
            "42315 82429\n",
            "42350 82429\n",
            "42385 82429\n",
            "42420 82429\n",
            "42455 82429\n",
            "42490 82429\n",
            "42525 82429\n",
            "42560 82429\n",
            "42595 82429\n",
            "42630 82429\n",
            "42665 82429\n",
            "42700 82429\n",
            "42735 82429\n",
            "42770 82429\n",
            "42805 82429\n",
            "42840 82429\n",
            "42875 82429\n",
            "42910 82429\n",
            "42945 82429\n",
            "42980 82429\n",
            "43015 82429\n",
            "43050 82429\n",
            "43085 82429\n",
            "43120 82429\n",
            "43155 82429\n",
            "43190 82429\n",
            "43225 82429\n",
            "43260 82429\n",
            "43295 82429\n",
            "43330 82429\n",
            "43365 82429\n",
            "43400 82429\n",
            "43435 82429\n",
            "43470 82429\n",
            "43505 82429\n",
            "43540 82429\n",
            "43575 82429\n",
            "43610 82429\n",
            "43645 82429\n",
            "43680 82429\n",
            "43715 82429\n",
            "43750 82429\n",
            "43785 82429\n",
            "43820 82429\n",
            "43855 82429\n",
            "43890 82429\n",
            "43925 82429\n",
            "43960 82429\n",
            "43995 82429\n",
            "44030 82429\n",
            "44065 82429\n",
            "44100 82429\n",
            "44135 82429\n",
            "44170 82429\n",
            "44205 82429\n",
            "44240 82429\n",
            "44275 82429\n",
            "44310 82429\n",
            "44345 82429\n",
            "44380 82429\n",
            "44415 82429\n",
            "44450 82429\n",
            "44485 82429\n",
            "44520 82429\n",
            "44555 82429\n",
            "44590 82429\n",
            "44625 82429\n",
            "44660 82429\n",
            "44695 82429\n",
            "44730 82429\n",
            "44765 82429\n",
            "44800 82429\n",
            "44835 82429\n",
            "44870 82429\n",
            "44905 82429\n",
            "44940 82429\n",
            "44975 82429\n",
            "45010 82429\n",
            "45045 82429\n",
            "45080 82429\n",
            "45115 82429\n",
            "45150 82429\n",
            "45185 82429\n",
            "45220 82429\n",
            "45255 82429\n",
            "45290 82429\n",
            "45325 82429\n",
            "45360 82429\n",
            "45395 82429\n",
            "45430 82429\n",
            "45465 82429\n",
            "45500 82429\n",
            "45535 82429\n",
            "45570 82429\n",
            "45605 82429\n",
            "45640 82429\n",
            "45675 82429\n",
            "45710 82429\n",
            "45745 82429\n",
            "45780 82429\n",
            "45815 82429\n",
            "45850 82429\n",
            "45885 82429\n",
            "45920 82429\n",
            "45955 82429\n",
            "45990 82429\n",
            "46025 82429\n",
            "46060 82429\n",
            "46095 82429\n",
            "46130 82429\n",
            "46165 82429\n",
            "46200 82429\n",
            "46235 82429\n",
            "46270 82429\n",
            "46305 82429\n",
            "46340 82429\n",
            "46375 82429\n",
            "46410 82429\n",
            "46445 82429\n",
            "46480 82429\n",
            "46515 82429\n",
            "46550 82429\n",
            "46585 82429\n",
            "46620 82429\n",
            "46655 82429\n",
            "46690 82429\n",
            "46725 82429\n",
            "46760 82429\n",
            "46795 82429\n",
            "46830 82429\n",
            "46865 82429\n",
            "46900 82429\n",
            "46935 82429\n",
            "46970 82429\n",
            "47005 82429\n",
            "47040 82429\n",
            "47075 82429\n",
            "47110 82429\n",
            "47145 82429\n",
            "47180 82429\n",
            "47215 82429\n",
            "47250 82429\n",
            "47285 82429\n",
            "47320 82429\n",
            "47355 82429\n",
            "47390 82429\n",
            "47425 82429\n",
            "47460 82429\n",
            "47495 82429\n",
            "47530 82429\n",
            "47565 82429\n",
            "47600 82429\n",
            "47635 82429\n",
            "47670 82429\n",
            "47705 82429\n",
            "47740 82429\n",
            "47775 82429\n",
            "47810 82429\n",
            "47845 82429\n",
            "47880 82429\n",
            "47915 82429\n",
            "47950 82429\n",
            "47985 82429\n",
            "48020 82429\n",
            "48055 82429\n",
            "48090 82429\n",
            "48125 82429\n",
            "48160 82429\n",
            "48195 82429\n",
            "48230 82429\n",
            "48265 82429\n",
            "48300 82429\n",
            "48335 82429\n",
            "48370 82429\n",
            "48405 82429\n",
            "48440 82429\n",
            "48475 82429\n",
            "48510 82429\n",
            "48545 82429\n",
            "48580 82429\n",
            "48615 82429\n",
            "48650 82429\n",
            "48685 82429\n",
            "48720 82429\n",
            "48755 82429\n",
            "48790 82429\n",
            "48825 82429\n",
            "48860 82429\n",
            "48895 82429\n",
            "48930 82429\n",
            "48965 82429\n",
            "49000 82429\n",
            "49035 82429\n",
            "49070 82429\n",
            "49105 82429\n",
            "49140 82429\n",
            "49175 82429\n",
            "49210 82429\n",
            "49245 82429\n",
            "49280 82429\n",
            "49315 82429\n",
            "49350 82429\n",
            "49385 82429\n",
            "49420 82429\n",
            "49455 82429\n",
            "49490 82429\n",
            "49525 82429\n",
            "49560 82429\n",
            "49595 82429\n",
            "49630 82429\n",
            "49665 82429\n",
            "49700 82429\n",
            "49735 82429\n",
            "49770 82429\n",
            "49805 82429\n",
            "49840 82429\n",
            "49875 82429\n",
            "49910 82429\n",
            "49945 82429\n",
            "49980 82429\n",
            "50015 82429\n",
            "50050 82429\n",
            "50085 82429\n",
            "50120 82429\n",
            "50155 82429\n",
            "50190 82429\n",
            "50225 82429\n",
            "50260 82429\n",
            "50295 82429\n",
            "50330 82429\n",
            "50365 82429\n",
            "50400 82429\n",
            "50435 82429\n",
            "50470 82429\n",
            "50505 82429\n",
            "50540 82429\n",
            "50575 82429\n",
            "50610 82429\n",
            "50645 82429\n",
            "50680 82429\n",
            "50715 82429\n",
            "50750 82429\n",
            "50785 82429\n",
            "50820 82429\n",
            "50855 82429\n",
            "50890 82429\n",
            "50925 82429\n",
            "50960 82429\n",
            "50995 82429\n",
            "51030 82429\n",
            "51065 82429\n",
            "51100 82429\n",
            "51135 82429\n",
            "51170 82429\n",
            "51205 82429\n",
            "51240 82429\n",
            "51275 82429\n",
            "51310 82429\n",
            "51345 82429\n",
            "51380 82429\n",
            "51415 82429\n",
            "51450 82429\n",
            "51485 82429\n",
            "51520 82429\n",
            "51555 82429\n",
            "51590 82429\n",
            "51625 82429\n",
            "51660 82429\n",
            "51695 82429\n",
            "51730 82429\n",
            "51765 82429\n",
            "51800 82429\n",
            "51835 82429\n",
            "51870 82429\n",
            "51905 82429\n",
            "51940 82429\n",
            "51975 82429\n",
            "52010 82429\n",
            "52045 82429\n",
            "52080 82429\n",
            "52115 82429\n",
            "52150 82429\n",
            "52185 82429\n",
            "52220 82429\n",
            "52255 82429\n",
            "52290 82429\n",
            "52325 82429\n",
            "52360 82429\n",
            "52395 82429\n",
            "52430 82429\n",
            "52465 82429\n",
            "52500 82429\n",
            "52535 82429\n",
            "52570 82429\n",
            "52605 82429\n",
            "52640 82429\n",
            "52675 82429\n",
            "52710 82429\n",
            "52745 82429\n",
            "52780 82429\n",
            "52815 82429\n",
            "52850 82429\n",
            "52885 82429\n",
            "52920 82429\n",
            "52955 82429\n",
            "52990 82429\n",
            "53025 82429\n",
            "53060 82429\n",
            "53095 82429\n",
            "53130 82429\n",
            "53165 82429\n",
            "53200 82429\n",
            "53235 82429\n",
            "53270 82429\n",
            "53305 82429\n",
            "53340 82429\n",
            "53375 82429\n",
            "53410 82429\n",
            "53445 82429\n",
            "53480 82429\n",
            "53515 82429\n",
            "53550 82429\n",
            "53585 82429\n",
            "53620 82429\n",
            "53655 82429\n",
            "53690 82429\n",
            "53725 82429\n",
            "53760 82429\n",
            "53795 82429\n",
            "53830 82429\n",
            "53865 82429\n",
            "53900 82429\n",
            "53935 82429\n",
            "53970 82429\n",
            "54005 82429\n",
            "54040 82429\n",
            "54075 82429\n",
            "54110 82429\n",
            "54145 82429\n",
            "54180 82429\n",
            "54215 82429\n",
            "54250 82429\n",
            "54285 82429\n",
            "54320 82429\n",
            "54355 82429\n",
            "54390 82429\n",
            "54425 82429\n",
            "54460 82429\n",
            "54495 82429\n",
            "54530 82429\n",
            "54565 82429\n",
            "54600 82429\n",
            "54635 82429\n",
            "54670 82429\n",
            "54705 82429\n",
            "54740 82429\n",
            "54775 82429\n",
            "54810 82429\n",
            "54845 82429\n",
            "54880 82429\n",
            "54915 82429\n",
            "54950 82429\n",
            "54985 82429\n",
            "55020 82429\n",
            "55055 82429\n",
            "55090 82429\n",
            "55125 82429\n",
            "55160 82429\n",
            "55195 82429\n",
            "55230 82429\n",
            "55265 82429\n",
            "55300 82429\n",
            "55335 82429\n",
            "55370 82429\n",
            "55405 82429\n",
            "55440 82429\n",
            "55475 82429\n",
            "55510 82429\n",
            "55545 82429\n",
            "55580 82429\n",
            "55615 82429\n",
            "55650 82429\n",
            "55685 82429\n",
            "55720 82429\n",
            "55755 82429\n",
            "55790 82429\n",
            "55825 82429\n",
            "55860 82429\n",
            "55895 82429\n",
            "55930 82429\n",
            "55965 82429\n",
            "56000 82429\n",
            "56035 82429\n",
            "56070 82429\n",
            "56105 82429\n",
            "56140 82429\n",
            "56175 82429\n",
            "56210 82429\n",
            "56245 82429\n",
            "56280 82429\n",
            "56315 82429\n",
            "56350 82429\n",
            "56385 82429\n",
            "56420 82429\n",
            "56455 82429\n",
            "56490 82429\n",
            "56525 82429\n",
            "56560 82429\n",
            "56595 82429\n",
            "56630 82429\n",
            "56665 82429\n",
            "56700 82429\n",
            "56735 82429\n",
            "56770 82429\n",
            "56805 82429\n",
            "56840 82429\n",
            "56875 82429\n",
            "56910 82429\n",
            "56945 82429\n",
            "56980 82429\n",
            "57015 82429\n",
            "57050 82429\n",
            "57085 82429\n",
            "57120 82429\n",
            "57155 82429\n",
            "57190 82429\n",
            "57225 82429\n",
            "57260 82429\n",
            "57295 82429\n",
            "57330 82429\n",
            "57365 82429\n",
            "57400 82429\n",
            "57435 82429\n",
            "57470 82429\n",
            "57505 82429\n",
            "57540 82429\n",
            "57575 82429\n",
            "57610 82429\n",
            "57645 82429\n",
            "57680 82429\n",
            "57715 82429\n",
            "57750 82429\n",
            "57785 82429\n",
            "57820 82429\n",
            "57855 82429\n",
            "57890 82429\n",
            "57925 82429\n",
            "57960 82429\n",
            "57995 82429\n",
            "58030 82429\n",
            "58065 82429\n",
            "58100 82429\n",
            "58135 82429\n",
            "58170 82429\n",
            "58205 82429\n",
            "58240 82429\n",
            "58275 82429\n",
            "58310 82429\n",
            "58345 82429\n",
            "58380 82429\n",
            "58415 82429\n",
            "58450 82429\n",
            "58485 82429\n",
            "58520 82429\n",
            "58555 82429\n",
            "58590 82429\n",
            "58625 82429\n",
            "58660 82429\n",
            "58695 82429\n",
            "58730 82429\n",
            "58765 82429\n",
            "58800 82429\n",
            "58835 82429\n",
            "58870 82429\n",
            "58905 82429\n",
            "58940 82429\n",
            "58975 82429\n",
            "59010 82429\n",
            "59045 82429\n",
            "59080 82429\n",
            "59115 82429\n",
            "59150 82429\n",
            "59185 82429\n",
            "59220 82429\n",
            "59255 82429\n",
            "59290 82429\n",
            "59325 82429\n",
            "59360 82429\n",
            "59395 82429\n",
            "59430 82429\n",
            "59465 82429\n",
            "59500 82429\n",
            "59535 82429\n",
            "59570 82429\n",
            "59605 82429\n",
            "59640 82429\n",
            "59675 82429\n",
            "59710 82429\n",
            "59745 82429\n",
            "59780 82429\n",
            "59815 82429\n",
            "59850 82429\n",
            "59885 82429\n",
            "59920 82429\n",
            "59955 82429\n",
            "59990 82429\n",
            "60025 82429\n",
            "60060 82429\n",
            "60095 82429\n",
            "60130 82429\n",
            "60165 82429\n",
            "60200 82429\n",
            "60235 82429\n",
            "60270 82429\n",
            "60305 82429\n",
            "60340 82429\n",
            "60375 82429\n",
            "60410 82429\n",
            "60445 82429\n",
            "60480 82429\n",
            "60515 82429\n",
            "60550 82429\n",
            "60585 82429\n",
            "60620 82429\n",
            "60655 82429\n",
            "60690 82429\n",
            "60725 82429\n",
            "60760 82429\n",
            "60795 82429\n",
            "60830 82429\n",
            "60865 82429\n",
            "60900 82429\n",
            "60935 82429\n",
            "60970 82429\n",
            "61005 82429\n",
            "61040 82429\n",
            "61075 82429\n",
            "61110 82429\n",
            "61145 82429\n",
            "61180 82429\n",
            "61215 82429\n",
            "61250 82429\n",
            "61285 82429\n",
            "61320 82429\n",
            "61355 82429\n",
            "61390 82429\n",
            "61425 82429\n",
            "61460 82429\n",
            "61495 82429\n",
            "61530 82429\n",
            "61565 82429\n",
            "61600 82429\n",
            "61635 82429\n",
            "61670 82429\n",
            "61705 82429\n",
            "61740 82429\n",
            "61775 82429\n",
            "61810 82429\n",
            "61845 82429\n",
            "61880 82429\n",
            "61915 82429\n",
            "61950 82429\n",
            "61985 82429\n",
            "62020 82429\n",
            "62055 82429\n",
            "62090 82429\n",
            "62125 82429\n",
            "62160 82429\n",
            "62195 82429\n",
            "62230 82429\n",
            "62265 82429\n",
            "62300 82429\n",
            "62335 82429\n",
            "62370 82429\n",
            "62405 82429\n",
            "62440 82429\n",
            "62475 82429\n",
            "62510 82429\n",
            "62545 82429\n",
            "62580 82429\n",
            "62615 82429\n",
            "62650 82429\n",
            "62685 82429\n",
            "62720 82429\n",
            "62755 82429\n",
            "62790 82429\n",
            "62825 82429\n",
            "62860 82429\n",
            "62895 82429\n",
            "62930 82429\n",
            "62965 82429\n",
            "63000 82429\n",
            "63035 82429\n",
            "63070 82429\n",
            "63105 82429\n",
            "63140 82429\n",
            "63175 82429\n",
            "63210 82429\n",
            "63245 82429\n",
            "63280 82429\n",
            "63315 82429\n",
            "63350 82429\n",
            "63385 82429\n",
            "63420 82429\n",
            "63455 82429\n",
            "63490 82429\n",
            "63525 82429\n",
            "63560 82429\n",
            "63595 82429\n",
            "63630 82429\n",
            "63665 82429\n",
            "63700 82429\n",
            "63735 82429\n",
            "63770 82429\n",
            "63805 82429\n",
            "63840 82429\n",
            "63875 82429\n",
            "63910 82429\n",
            "63945 82429\n",
            "63980 82429\n",
            "64015 82429\n",
            "64050 82429\n",
            "64085 82429\n",
            "64120 82429\n",
            "64155 82429\n",
            "64190 82429\n",
            "64225 82429\n",
            "64260 82429\n",
            "64295 82429\n",
            "64330 82429\n",
            "64365 82429\n",
            "64400 82429\n",
            "64435 82429\n",
            "64470 82429\n",
            "64505 82429\n",
            "64540 82429\n",
            "64575 82429\n",
            "64610 82429\n",
            "64645 82429\n",
            "64680 82429\n",
            "64715 82429\n",
            "64750 82429\n",
            "64785 82429\n",
            "64820 82429\n",
            "64855 82429\n",
            "64890 82429\n",
            "64925 82429\n",
            "64960 82429\n",
            "64995 82429\n",
            "65030 82429\n",
            "65065 82429\n",
            "65100 82429\n",
            "65135 82429\n",
            "65170 82429\n",
            "65205 82429\n",
            "65240 82429\n",
            "65275 82429\n",
            "65310 82429\n",
            "65345 82429\n",
            "65380 82429\n",
            "65415 82429\n",
            "65450 82429\n",
            "65485 82429\n",
            "65520 82429\n",
            "65555 82429\n",
            "65590 82429\n",
            "65625 82429\n",
            "65660 82429\n",
            "65695 82429\n",
            "65730 82429\n",
            "65765 82429\n",
            "65800 82429\n",
            "65835 82429\n",
            "65870 82429\n",
            "65905 82429\n",
            "65940 82429\n",
            "65975 82429\n",
            "66010 82429\n",
            "66045 82429\n",
            "66080 82429\n",
            "66115 82429\n",
            "66150 82429\n",
            "66185 82429\n",
            "66220 82429\n",
            "66255 82429\n",
            "66290 82429\n",
            "66325 82429\n",
            "66360 82429\n",
            "66395 82429\n",
            "66430 82429\n",
            "66465 82429\n",
            "66500 82429\n",
            "66535 82429\n",
            "66570 82429\n",
            "66605 82429\n",
            "66640 82429\n",
            "66675 82429\n",
            "66710 82429\n",
            "66745 82429\n",
            "66780 82429\n",
            "66815 82429\n",
            "66850 82429\n",
            "66885 82429\n",
            "66920 82429\n",
            "66955 82429\n",
            "66990 82429\n",
            "67025 82429\n",
            "67060 82429\n",
            "67095 82429\n",
            "67130 82429\n",
            "67165 82429\n",
            "67200 82429\n",
            "67235 82429\n",
            "67270 82429\n",
            "67305 82429\n",
            "67340 82429\n",
            "67375 82429\n",
            "67410 82429\n",
            "67445 82429\n",
            "67480 82429\n",
            "67515 82429\n",
            "67550 82429\n",
            "67585 82429\n",
            "67620 82429\n",
            "67655 82429\n",
            "67690 82429\n",
            "67725 82429\n",
            "67760 82429\n",
            "67795 82429\n",
            "67830 82429\n",
            "67865 82429\n",
            "67900 82429\n",
            "67935 82429\n",
            "67970 82429\n",
            "68005 82429\n",
            "68040 82429\n",
            "68075 82429\n",
            "68110 82429\n",
            "68145 82429\n",
            "68180 82429\n",
            "68215 82429\n",
            "68250 82429\n",
            "68285 82429\n",
            "68320 82429\n",
            "68355 82429\n",
            "68390 82429\n",
            "68425 82429\n",
            "68460 82429\n",
            "68495 82429\n",
            "68530 82429\n",
            "68565 82429\n",
            "68600 82429\n",
            "68635 82429\n",
            "68670 82429\n",
            "68705 82429\n",
            "68740 82429\n",
            "68775 82429\n",
            "68810 82429\n",
            "68845 82429\n",
            "68880 82429\n",
            "68915 82429\n",
            "68950 82429\n",
            "68985 82429\n",
            "69020 82429\n",
            "69055 82429\n",
            "69090 82429\n",
            "69125 82429\n",
            "69160 82429\n",
            "69195 82429\n",
            "69230 82429\n",
            "69265 82429\n",
            "69300 82429\n",
            "69335 82429\n",
            "69370 82429\n",
            "69405 82429\n",
            "69440 82429\n",
            "69475 82429\n",
            "69510 82429\n",
            "69545 82429\n",
            "69580 82429\n",
            "69615 82429\n",
            "69650 82429\n",
            "69685 82429\n",
            "69720 82429\n",
            "69755 82429\n",
            "69790 82429\n",
            "69825 82429\n",
            "69860 82429\n",
            "69895 82429\n",
            "69930 82429\n",
            "69965 82429\n",
            "70000 82429\n",
            "70035 82429\n",
            "70070 82429\n",
            "70105 82429\n",
            "70140 82429\n",
            "70175 82429\n",
            "70210 82429\n",
            "70245 82429\n",
            "70280 82429\n",
            "70315 82429\n",
            "70350 82429\n",
            "70385 82429\n",
            "70420 82429\n",
            "70455 82429\n",
            "70490 82429\n",
            "70525 82429\n",
            "70560 82429\n",
            "70595 82429\n",
            "70630 82429\n",
            "70665 82429\n",
            "70700 82429\n",
            "70735 82429\n",
            "70770 82429\n",
            "70805 82429\n",
            "70840 82429\n",
            "70875 82429\n",
            "70910 82429\n",
            "70945 82429\n",
            "70980 82429\n",
            "71015 82429\n",
            "71050 82429\n",
            "71085 82429\n",
            "71120 82429\n",
            "71155 82429\n",
            "71190 82429\n",
            "71225 82429\n",
            "71260 82429\n",
            "71295 82429\n",
            "71330 82429\n",
            "71365 82429\n",
            "71400 82429\n",
            "71435 82429\n",
            "71470 82429\n",
            "71505 82429\n",
            "71540 82429\n",
            "71575 82429\n",
            "71610 82429\n",
            "71645 82429\n",
            "71680 82429\n",
            "71715 82429\n",
            "71750 82429\n",
            "71785 82429\n",
            "71820 82429\n",
            "71855 82429\n",
            "71890 82429\n",
            "71925 82429\n",
            "71960 82429\n",
            "71995 82429\n",
            "72030 82429\n",
            "72065 82429\n",
            "72100 82429\n",
            "72135 82429\n",
            "72170 82429\n",
            "72205 82429\n",
            "72240 82429\n",
            "72275 82429\n",
            "72310 82429\n",
            "72345 82429\n",
            "72380 82429\n",
            "72415 82429\n",
            "72450 82429\n",
            "72485 82429\n",
            "72520 82429\n",
            "72555 82429\n",
            "72590 82429\n",
            "72625 82429\n",
            "72660 82429\n",
            "72695 82429\n",
            "72730 82429\n",
            "72765 82429\n",
            "72800 82429\n",
            "72835 82429\n",
            "72870 82429\n",
            "72905 82429\n",
            "72940 82429\n",
            "72975 82429\n",
            "73010 82429\n",
            "73045 82429\n",
            "73080 82429\n",
            "73115 82429\n",
            "73150 82429\n",
            "73185 82429\n",
            "73220 82429\n",
            "73255 82429\n",
            "73290 82429\n",
            "73325 82429\n",
            "73360 82429\n",
            "73395 82429\n",
            "73430 82429\n",
            "73465 82429\n",
            "73500 82429\n",
            "73535 82429\n",
            "73570 82429\n",
            "73605 82429\n",
            "73640 82429\n",
            "73675 82429\n",
            "73710 82429\n",
            "73745 82429\n",
            "73780 82429\n",
            "73815 82429\n",
            "73850 82429\n",
            "73885 82429\n",
            "73920 82429\n",
            "73955 82429\n",
            "73990 82429\n",
            "74025 82429\n",
            "74060 82429\n",
            "74095 82429\n",
            "74130 82429\n",
            "74165 82429\n",
            "74200 82429\n",
            "74235 82429\n",
            "74270 82429\n",
            "74305 82429\n",
            "74340 82429\n",
            "74375 82429\n",
            "74410 82429\n",
            "74445 82429\n",
            "74480 82429\n",
            "74515 82429\n",
            "74550 82429\n",
            "74585 82429\n",
            "74620 82429\n",
            "74655 82429\n",
            "74690 82429\n",
            "74725 82429\n",
            "74760 82429\n",
            "74795 82429\n",
            "74830 82429\n",
            "74865 82429\n",
            "74900 82429\n",
            "74935 82429\n",
            "74970 82429\n",
            "75005 82429\n",
            "75040 82429\n",
            "75075 82429\n",
            "75110 82429\n",
            "75145 82429\n",
            "75180 82429\n",
            "75215 82429\n",
            "75250 82429\n",
            "75285 82429\n",
            "75320 82429\n",
            "75355 82429\n",
            "75390 82429\n",
            "75425 82429\n",
            "75460 82429\n",
            "75495 82429\n",
            "75530 82429\n",
            "75565 82429\n",
            "75600 82429\n",
            "75635 82429\n",
            "75670 82429\n",
            "75705 82429\n",
            "75740 82429\n",
            "75775 82429\n",
            "75810 82429\n",
            "75845 82429\n",
            "75880 82429\n",
            "75915 82429\n",
            "75950 82429\n",
            "75985 82429\n",
            "76020 82429\n",
            "76055 82429\n",
            "76090 82429\n",
            "76125 82429\n",
            "76160 82429\n",
            "76195 82429\n",
            "76230 82429\n",
            "76265 82429\n",
            "76300 82429\n",
            "76335 82429\n",
            "76370 82429\n",
            "76405 82429\n",
            "76440 82429\n",
            "76475 82429\n",
            "76510 82429\n",
            "76545 82429\n",
            "76580 82429\n",
            "76615 82429\n",
            "76650 82429\n",
            "76685 82429\n",
            "76720 82429\n",
            "76755 82429\n",
            "76790 82429\n",
            "76825 82429\n",
            "76860 82429\n",
            "76895 82429\n",
            "76930 82429\n",
            "76965 82429\n",
            "77000 82429\n",
            "77035 82429\n",
            "77070 82429\n",
            "77105 82429\n",
            "77140 82429\n",
            "77175 82429\n",
            "77210 82429\n",
            "77245 82429\n",
            "77280 82429\n",
            "77315 82429\n",
            "77350 82429\n",
            "77385 82429\n",
            "77420 82429\n",
            "77455 82429\n",
            "77490 82429\n",
            "77525 82429\n",
            "77560 82429\n",
            "77595 82429\n",
            "77630 82429\n",
            "77665 82429\n",
            "77700 82429\n",
            "77735 82429\n",
            "77770 82429\n",
            "77805 82429\n",
            "77840 82429\n",
            "77875 82429\n",
            "77910 82429\n",
            "77945 82429\n",
            "77980 82429\n",
            "78015 82429\n",
            "78050 82429\n",
            "78085 82429\n",
            "78120 82429\n",
            "78155 82429\n",
            "78190 82429\n",
            "78225 82429\n",
            "78260 82429\n",
            "78295 82429\n",
            "78330 82429\n",
            "78365 82429\n",
            "78400 82429\n",
            "78435 82429\n",
            "78470 82429\n",
            "78505 82429\n",
            "78540 82429\n",
            "78575 82429\n",
            "78610 82429\n",
            "78645 82429\n",
            "78680 82429\n",
            "78715 82429\n",
            "78750 82429\n",
            "78785 82429\n",
            "78820 82429\n",
            "78855 82429\n",
            "78890 82429\n",
            "78925 82429\n",
            "78960 82429\n",
            "78995 82429\n",
            "79030 82429\n",
            "79065 82429\n",
            "79100 82429\n",
            "79135 82429\n",
            "79170 82429\n",
            "79205 82429\n",
            "79240 82429\n",
            "79275 82429\n",
            "79310 82429\n",
            "79345 82429\n",
            "79380 82429\n",
            "79415 82429\n",
            "79450 82429\n",
            "79485 82429\n",
            "79520 82429\n",
            "79555 82429\n",
            "79590 82429\n",
            "79625 82429\n",
            "79660 82429\n",
            "79695 82429\n",
            "79730 82429\n",
            "79765 82429\n",
            "79800 82429\n",
            "79835 82429\n",
            "79870 82429\n",
            "79905 82429\n",
            "79940 82429\n",
            "79975 82429\n",
            "80010 82429\n",
            "80045 82429\n",
            "80080 82429\n",
            "80115 82429\n",
            "80150 82429\n",
            "80185 82429\n",
            "80220 82429\n",
            "80255 82429\n",
            "80290 82429\n",
            "80325 82429\n",
            "80360 82429\n",
            "80395 82429\n",
            "80430 82429\n",
            "80465 82429\n",
            "80500 82429\n",
            "80535 82429\n",
            "80570 82429\n",
            "80605 82429\n",
            "80640 82429\n",
            "80675 82429\n",
            "80710 82429\n",
            "80745 82429\n",
            "80780 82429\n",
            "80815 82429\n",
            "80850 82429\n",
            "80885 82429\n",
            "80920 82429\n",
            "80955 82429\n",
            "80990 82429\n",
            "81025 82429\n",
            "81060 82429\n",
            "81095 82429\n",
            "81130 82429\n",
            "81165 82429\n",
            "81200 82429\n",
            "81235 82429\n",
            "81270 82429\n",
            "81305 82429\n",
            "81340 82429\n",
            "81375 82429\n",
            "81410 82429\n",
            "81445 82429\n",
            "81480 82429\n",
            "81515 82429\n",
            "81550 82429\n",
            "81585 82429\n",
            "81620 82429\n",
            "81655 82429\n",
            "81690 82429\n",
            "81725 82429\n",
            "81760 82429\n",
            "81795 82429\n",
            "81830 82429\n",
            "81865 82429\n",
            "81900 82429\n",
            "81935 82429\n",
            "81970 82429\n",
            "82005 82429\n",
            "82040 82429\n",
            "82075 82429\n",
            "82110 82429\n",
            "82145 82429\n",
            "82180 82429\n",
            "82215 82429\n",
            "82250 82429\n",
            "82285 82429\n",
            "82320 82429\n",
            "82355 82429\n",
            "82390 82429\n",
            "82425 82429\n",
            "=========================================================================================\n",
            "| End of training | test loss  4.02 | test ppl    55.68\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "trainset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/B19EE046_Q4/darts/data/cifar10', train=True,download=True)\n",
        "testset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/B19EE046_Q4/darts/data/cifar10', train=False,download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlIS2TszVUWd",
        "outputId": "85e46bdb-0f1f-458e-b0db-edfa9daed9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --auxiliary --model_path '/content/drive/MyDrive/B19EE046_Q4/darts/cnn/cifar10_model.pt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQmypsSqWjz-",
        "outputId": "5a32638a-3bb0-4902-d1cc-c4edb41d5de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04/28 06:21:23 PM gpu device = 0\n",
            "04/28 06:21:23 PM args = Namespace(arch='DARTS', auxiliary=True, batch_size=96, cutout=False, cutout_length=16, data='../data', drop_path_prob=0.2, gpu=0, init_channels=36, layers=20, model_path='/content/drive/MyDrive/B19EE046_Q4/darts/cnn/cifar10_model.pt', report_freq=50, seed=0)\n",
            "108 108 36\n",
            "108 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 72\n",
            "144 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 144\n",
            "288 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "04/28 06:21:27 PM param size = 3.349342MB\n",
            "Files already downloaded and verified\n",
            "test.py:84: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input = Variable(input, volatile=True).cuda()\n",
            "test.py:85: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  target = Variable(target, volatile=True).cuda()\n",
            "04/28 06:21:29 PM test 000 1.233735e-01 96.875000 100.000000\n",
            "04/28 06:21:34 PM test 050 1.105460e-01 97.120095 99.959150\n",
            "04/28 06:21:39 PM test 100 1.074740e-01 97.359733 99.948432\n",
            "04/28 06:21:39 PM test_acc 97.369997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_search.py --unrolled "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEyYfme3aLwL",
        "outputId": "b2930c10-1fcf-4a35-d3cb-ba84965c3b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment dir : search-EXP-20220428-192755\n",
            "04/28 07:27:55 PM gpu device = 0\n",
            "04/28 07:27:55 PM args = Namespace(arch_learning_rate=0.0003, arch_weight_decay=0.001, batch_size=64, cutout=False, cutout_length=16, data='../data', drop_path_prob=0.3, epochs=50, gpu=0, grad_clip=5, init_channels=16, layers=8, learning_rate=0.025, learning_rate_min=0.001, model_path='saved_models', momentum=0.9, report_freq=50, save='search-EXP-20220428-192755', seed=2, train_portion=0.5, unrolled=True, weight_decay=0.0003)\n",
            "04/28 07:27:59 PM param size = 1.930618MB\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:729: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n",
            "04/28 07:28:00 PM epoch 0 lr 2.495266e-02\n",
            "04/28 07:28:00 PM genotype = Genotype(normal=[('dil_conv_5x5', 1), ('sep_conv_3x3', 0), ('dil_conv_3x3', 2), ('dil_conv_5x5', 1), ('avg_pool_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_3x3', 0), ('max_pool_3x3', 3)], normal_concat=range(2, 6), reduce=[('avg_pool_3x3', 1), ('sep_conv_5x5', 0), ('dil_conv_5x5', 0), ('sep_conv_3x3', 1), ('max_pool_3x3', 0), ('avg_pool_3x3', 2), ('sep_conv_3x3', 4), ('sep_conv_3x3', 0)], reduce_concat=range(2, 6))\n",
            "tensor([[0.1250, 0.1247, 0.1251, 0.1250, 0.1251, 0.1251, 0.1250, 0.1250],\n",
            "        [0.1249, 0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1251, 0.1251],\n",
            "        [0.1251, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1248, 0.1251],\n",
            "        [0.1250, 0.1250, 0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1251],\n",
            "        [0.1248, 0.1250, 0.1249, 0.1249, 0.1249, 0.1251, 0.1252, 0.1251],\n",
            "        [0.1249, 0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1250, 0.1250],\n",
            "        [0.1249, 0.1249, 0.1252, 0.1251, 0.1250, 0.1250, 0.1250, 0.1249],\n",
            "        [0.1250, 0.1248, 0.1251, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249],\n",
            "        [0.1249, 0.1249, 0.1249, 0.1250, 0.1249, 0.1250, 0.1253, 0.1251],\n",
            "        [0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249, 0.1251],\n",
            "        [0.1250, 0.1250, 0.1252, 0.1251, 0.1249, 0.1249, 0.1250, 0.1249],\n",
            "        [0.1249, 0.1253, 0.1250, 0.1248, 0.1248, 0.1251, 0.1251, 0.1250],\n",
            "        [0.1249, 0.1251, 0.1251, 0.1252, 0.1250, 0.1248, 0.1250, 0.1249]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.1251, 0.1250, 0.1250, 0.1250, 0.1249, 0.1251, 0.1249, 0.1250],\n",
            "        [0.1249, 0.1248, 0.1252, 0.1247, 0.1251, 0.1249, 0.1252, 0.1251],\n",
            "        [0.1249, 0.1249, 0.1251, 0.1250, 0.1248, 0.1250, 0.1249, 0.1254],\n",
            "        [0.1251, 0.1250, 0.1250, 0.1250, 0.1252, 0.1250, 0.1249, 0.1250],\n",
            "        [0.1251, 0.1249, 0.1250, 0.1250, 0.1251, 0.1249, 0.1250, 0.1251],\n",
            "        [0.1251, 0.1252, 0.1251, 0.1247, 0.1252, 0.1249, 0.1249, 0.1250],\n",
            "        [0.1251, 0.1251, 0.1251, 0.1250, 0.1247, 0.1250, 0.1251, 0.1250],\n",
            "        [0.1250, 0.1249, 0.1251, 0.1250, 0.1250, 0.1248, 0.1251, 0.1248],\n",
            "        [0.1252, 0.1251, 0.1250, 0.1250, 0.1249, 0.1249, 0.1250, 0.1250],\n",
            "        [0.1248, 0.1249, 0.1250, 0.1249, 0.1252, 0.1250, 0.1251, 0.1251],\n",
            "        [0.1252, 0.1249, 0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1251],\n",
            "        [0.1249, 0.1251, 0.1250, 0.1250, 0.1250, 0.1251, 0.1250, 0.1249],\n",
            "        [0.1249, 0.1249, 0.1251, 0.1251, 0.1246, 0.1251, 0.1251, 0.1251],\n",
            "        [0.1250, 0.1247, 0.1250, 0.1251, 0.1252, 0.1250, 0.1250, 0.1250]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "/content/drive/MyDrive/B19EE046_Q4/darts/cnn/architect.py:28: UserWarning: This overload of sub is deprecated:\n",
            "\tsub(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tsub(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
            "  unrolled_model = self._construct_model_from_theta(theta.sub(eta, moment+dtheta))\n",
            "train_search.py:154: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\n",
            "04/28 07:28:10 PM train 000 2.475923e+00 9.375000 43.750000\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "04/28 07:34:56 PM train 050 2.017678e+00 25.275735 78.829657\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "04/28 07:41:41 PM train 100 1.888249e+00 30.058787 82.750619\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "04/28 07:48:21 PM train 150 1.795865e+00 33.681705 84.778560\n",
            "04/28 07:54:58 PM train 200 1.737753e+00 35.665423 86.310634\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "04/28 08:01:35 PM train 250 1.680251e+00 37.842380 87.506225\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "04/28 08:08:12 PM train 300 1.636844e+00 39.498547 88.434385\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "04/28 08:14:48 PM train 350 1.592770e+00 41.132479 89.258369\n",
            "04/28 08:20:06 PM train_acc 41.980000\n",
            "train_search.py:175: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input = Variable(input, volatile=True).cuda()\n",
            "train_search.py:176: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  target = Variable(target, volatile=True).cuda()\n",
            "04/28 08:20:07 PM valid 000 1.446269e+00 50.000000 90.625000\n",
            "Traceback (most recent call last):\n",
            "  File \"train_search.py\", line 194, in <module>\n",
            "    main() \n",
            "  File \"train_search.py\", line 124, in main\n",
            "    valid_acc, valid_obj = infer(valid_queue, model, criterion)\n",
            "  File \"train_search.py\", line 178, in infer\n",
            "    logits = model(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/B19EE046_Q4/darts/cnn/model_search.py\", line 110, in forward\n",
            "    s0, s1 = s1, cell(s0, s1, weights)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/B19EE046_Q4/darts/cnn/model_search.py\", line 54, in forward\n",
            "    s = sum(self._ops[offset+j](h, weights[offset+j]) for j, h in enumerate(states))\n",
            "  File \"/content/drive/MyDrive/B19EE046_Q4/darts/cnn/model_search.py\", line 54, in <genexpr>\n",
            "    s = sum(self._ops[offset+j](h, weights[offset+j]) for j, h in enumerate(states))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/B19EE046_Q4/darts/cnn/model_search.py\", line 22, in forward\n",
            "    return sum(w * op(x) for w, op in zip(weights, self._ops))\n",
            "  File \"/content/drive/MyDrive/B19EE046_Q4/darts/cnn/model_search.py\", line 22, in <genexpr>\n",
            "    return sum(w * op(x) for w, op in zip(weights, self._ops))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/B19EE046_Q4/darts/cnn/operations.py\", line 66, in forward\n",
            "    return self.op(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 141, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\", line 179, in forward\n",
            "    self.eps,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 2422, in batch_norm\n",
            "    input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.90 GiB total capacity; 14.93 GiB already allocated; 21.75 MiB free; 14.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_search.py --data '/content/drive/MyDrive/B19EE046_Q4/awd-lstm-lm/data/penn/' --unrolled "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEXpIqKH1ZP1",
        "outputId": "eee07bf5-3da2-4698-b328-dec8e23ba51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment dir : search-EXP-20220428-202935\n",
            "torch.Size([3631, 256])\n",
            "torch.Size([288, 256])\n",
            "torch.Size([7376, 10])\n",
            "torch.Size([82430, 1])\n",
            "04/28 08:29:43 PM param size: 4810000\n",
            "04/28 08:29:43 PM initial genotype:\n",
            "04/28 08:29:43 PM Genotype(recurrent=[('identity', 0), ('relu', 1), ('identity', 0), ('identity', 2), ('sigmoid', 4), ('tanh', 0), ('sigmoid', 1), ('identity', 6)], concat=range(1, 9))\n",
            "04/28 08:29:43 PM Args: Namespace(alpha=0, arch_lr=0.003, arch_wdecay=0.001, batch_size=256, beta=0.001, bptt=35, clip=0.25, continue_train=False, cuda=True, data='/content/drive/MyDrive/B19EE046_Q4/awd-lstm-lm/data/penn/', dropout=0.75, dropoute=0, dropouth=0.25, dropouti=0.2, dropoutx=0.75, emsize=300, epochs=50, gpu=0, log_interval=50, lr=20, max_seq_len_delta=20, nhid=300, nhidlast=300, nonmono=5, save='search-EXP-20220428-202935', seed=3, single_gpu=True, small_batch_size=256, unrolled=True, wdecay=5e-07)\n",
            "04/28 08:29:43 PM Model total parameters: 4810000\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/content/drive/MyDrive/B19EE046_Q4/darts/rnn/architect.py:38: UserWarning: This overload of sub is deprecated:\n",
            "\tsub(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tsub(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
            "  unrolled_model = self._construct_model_from_theta(theta.sub(eta, dtheta))\n",
            "train_search.py:236: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "04/28 08:32:31 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('sigmoid', 0), ('identity', 3), ('sigmoid', 3), ('identity', 5), ('tanh', 0), ('tanh', 7)], concat=range(1, 9))\n",
            "tensor([[0.1999, 0.2015, 0.1985, 0.2072, 0.1929],\n",
            "        [0.1920, 0.2098, 0.1992, 0.2070, 0.1920],\n",
            "        [0.2073, 0.2005, 0.1962, 0.2080, 0.1880],\n",
            "        [0.1916, 0.2020, 0.2039, 0.2053, 0.1972],\n",
            "        [0.2045, 0.1948, 0.2017, 0.2014, 0.1976],\n",
            "        [0.2048, 0.1949, 0.2009, 0.2028, 0.1966],\n",
            "        [0.1938, 0.2034, 0.1965, 0.2014, 0.2048],\n",
            "        [0.2058, 0.1977, 0.1960, 0.1974, 0.2031],\n",
            "        [0.2086, 0.1969, 0.1940, 0.1972, 0.2033],\n",
            "        [0.1911, 0.2040, 0.1975, 0.2007, 0.2068],\n",
            "        [0.1929, 0.2093, 0.1936, 0.2096, 0.1945],\n",
            "        [0.2081, 0.2067, 0.1882, 0.2072, 0.1898],\n",
            "        [0.2026, 0.2053, 0.1904, 0.2086, 0.1931],\n",
            "        [0.1982, 0.1998, 0.1971, 0.2097, 0.1953],\n",
            "        [0.2015, 0.2010, 0.1909, 0.2095, 0.1971],\n",
            "        [0.1959, 0.2074, 0.1939, 0.1915, 0.2112],\n",
            "        [0.2029, 0.2023, 0.1934, 0.1922, 0.2092],\n",
            "        [0.1983, 0.2032, 0.1969, 0.1906, 0.2110],\n",
            "        [0.1983, 0.2029, 0.1960, 0.1926, 0.2102],\n",
            "        [0.2050, 0.2014, 0.1907, 0.1955, 0.2074],\n",
            "        [0.2016, 0.1991, 0.1958, 0.1922, 0.2113],\n",
            "        [0.1891, 0.2081, 0.1956, 0.2050, 0.2021],\n",
            "        [0.2023, 0.2016, 0.1945, 0.2039, 0.1977],\n",
            "        [0.2063, 0.1985, 0.1948, 0.2035, 0.1969],\n",
            "        [0.1907, 0.2060, 0.2000, 0.2017, 0.2016],\n",
            "        [0.1978, 0.2070, 0.1926, 0.2034, 0.1992],\n",
            "        [0.2032, 0.2027, 0.1945, 0.2016, 0.1980],\n",
            "        [0.1987, 0.2060, 0.1947, 0.2064, 0.1942],\n",
            "        [0.1985, 0.2010, 0.1973, 0.2010, 0.2022],\n",
            "        [0.2046, 0.1984, 0.1960, 0.1998, 0.2013],\n",
            "        [0.2043, 0.1989, 0.1974, 0.1982, 0.2012],\n",
            "        [0.1990, 0.1991, 0.1988, 0.2016, 0.2015],\n",
            "        [0.1983, 0.2001, 0.2003, 0.2005, 0.2008],\n",
            "        [0.2061, 0.1958, 0.1969, 0.2003, 0.2009],\n",
            "        [0.1980, 0.2024, 0.1945, 0.2010, 0.2040],\n",
            "        [0.1935, 0.2045, 0.1967, 0.2011, 0.2043]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 08:32:31 PM | epoch   1 |    50/  103 batches | lr 20.00 | ms/batch 3354.35 | loss  9.65 | ppl 15526.53\n",
            "04/28 08:35:14 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('identity', 0), ('sigmoid', 4), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2023, 0.1928, 0.1987, 0.2060, 0.2002],\n",
            "        [0.1932, 0.2153, 0.1934, 0.2069, 0.1911],\n",
            "        [0.2045, 0.2065, 0.1903, 0.2088, 0.1899],\n",
            "        [0.1906, 0.2072, 0.1994, 0.2067, 0.1961],\n",
            "        [0.2068, 0.1973, 0.1937, 0.2019, 0.2003],\n",
            "        [0.2009, 0.2010, 0.1965, 0.2034, 0.1981],\n",
            "        [0.1969, 0.1969, 0.1957, 0.1994, 0.2111],\n",
            "        [0.2138, 0.1922, 0.1935, 0.1959, 0.2046],\n",
            "        [0.2026, 0.1938, 0.1944, 0.1985, 0.2107],\n",
            "        [0.1866, 0.2018, 0.1993, 0.2030, 0.2093],\n",
            "        [0.1985, 0.2028, 0.1958, 0.2086, 0.1943],\n",
            "        [0.2105, 0.2000, 0.1930, 0.2065, 0.1901],\n",
            "        [0.2055, 0.1997, 0.1926, 0.2070, 0.1952],\n",
            "        [0.1911, 0.1981, 0.2023, 0.2092, 0.1993],\n",
            "        [0.2016, 0.1976, 0.1920, 0.2107, 0.1981],\n",
            "        [0.2024, 0.2003, 0.1956, 0.1945, 0.2073],\n",
            "        [0.2045, 0.1959, 0.1952, 0.1927, 0.2116],\n",
            "        [0.1894, 0.1998, 0.2033, 0.1927, 0.2148],\n",
            "        [0.1897, 0.2031, 0.1974, 0.1943, 0.2154],\n",
            "        [0.2051, 0.1960, 0.1936, 0.1953, 0.2100],\n",
            "        [0.2144, 0.1869, 0.1959, 0.1925, 0.2103],\n",
            "        [0.1906, 0.2037, 0.1969, 0.2017, 0.2070],\n",
            "        [0.1999, 0.1994, 0.1969, 0.1960, 0.2078],\n",
            "        [0.2051, 0.1951, 0.1965, 0.2044, 0.1988],\n",
            "        [0.1810, 0.2076, 0.2031, 0.1982, 0.2101],\n",
            "        [0.2045, 0.2044, 0.1910, 0.2020, 0.1980],\n",
            "        [0.2093, 0.1976, 0.1936, 0.1988, 0.2007],\n",
            "        [0.2013, 0.2051, 0.1929, 0.2053, 0.1955],\n",
            "        [0.2014, 0.2015, 0.1942, 0.2001, 0.2029],\n",
            "        [0.2085, 0.2025, 0.1912, 0.2023, 0.1956],\n",
            "        [0.1947, 0.2050, 0.1969, 0.2014, 0.2021],\n",
            "        [0.1907, 0.2037, 0.1971, 0.2047, 0.2038],\n",
            "        [0.1974, 0.2002, 0.1990, 0.2015, 0.2019],\n",
            "        [0.2125, 0.1920, 0.1944, 0.2008, 0.2004],\n",
            "        [0.1921, 0.2052, 0.1942, 0.2035, 0.2050],\n",
            "        [0.2028, 0.2008, 0.1929, 0.2035, 0.1999]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 08:35:14 PM | epoch   1 |   100/  103 batches | lr 20.00 | ms/batch 3257.54 | loss  9.29 | ppl 10845.81\n",
            "04/28 08:36:26 PM -----------------------------------------------------------------------------------------\n",
            "04/28 08:36:26 PM | end of epoch   1 | time: 403.13s | valid loss  8.81 | valid ppl  6729.25\n",
            "04/28 08:36:26 PM -----------------------------------------------------------------------------------------\n",
            "04/28 08:36:26 PM Saving Normal!\n",
            "04/28 08:39:12 PM Genotype(recurrent=[('identity', 0), ('tanh', 0), ('tanh', 0), ('identity', 0), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.1994, 0.1949, 0.1999, 0.2011, 0.2048],\n",
            "        [0.1941, 0.2170, 0.1952, 0.1975, 0.1961],\n",
            "        [0.2038, 0.2090, 0.1881, 0.1991, 0.2000],\n",
            "        [0.1858, 0.2175, 0.1936, 0.2053, 0.1978],\n",
            "        [0.2072, 0.2032, 0.1892, 0.1989, 0.2016],\n",
            "        [0.2030, 0.2117, 0.1874, 0.1987, 0.1991],\n",
            "        [0.1901, 0.1926, 0.2018, 0.1963, 0.2193],\n",
            "        [0.2156, 0.1872, 0.2050, 0.1932, 0.1991],\n",
            "        [0.2053, 0.1856, 0.2021, 0.1927, 0.2142],\n",
            "        [0.1870, 0.1985, 0.2070, 0.2011, 0.2064],\n",
            "        [0.1921, 0.2006, 0.2008, 0.2014, 0.2051],\n",
            "        [0.2040, 0.1983, 0.2002, 0.2021, 0.1954],\n",
            "        [0.2112, 0.1923, 0.1955, 0.1991, 0.2019],\n",
            "        [0.1851, 0.1940, 0.2084, 0.2034, 0.2091],\n",
            "        [0.2107, 0.1904, 0.1947, 0.2051, 0.1991],\n",
            "        [0.2057, 0.1950, 0.1969, 0.1870, 0.2154],\n",
            "        [0.2048, 0.1901, 0.1995, 0.1834, 0.2223],\n",
            "        [0.1930, 0.1917, 0.2072, 0.1839, 0.2242],\n",
            "        [0.1912, 0.1931, 0.2047, 0.1842, 0.2267],\n",
            "        [0.2104, 0.1867, 0.1981, 0.1856, 0.2191],\n",
            "        [0.2070, 0.1853, 0.2028, 0.1843, 0.2206],\n",
            "        [0.1984, 0.2030, 0.1934, 0.1985, 0.2067],\n",
            "        [0.2017, 0.2008, 0.1936, 0.1911, 0.2128],\n",
            "        [0.2090, 0.1939, 0.1947, 0.1983, 0.2041],\n",
            "        [0.1832, 0.2039, 0.2045, 0.1923, 0.2161],\n",
            "        [0.2142, 0.2020, 0.1868, 0.1967, 0.2004],\n",
            "        [0.1986, 0.2014, 0.1938, 0.1982, 0.2080],\n",
            "        [0.1930, 0.2074, 0.1955, 0.2011, 0.2031],\n",
            "        [0.1932, 0.2072, 0.1913, 0.1954, 0.2129],\n",
            "        [0.2100, 0.2073, 0.1854, 0.1970, 0.2003],\n",
            "        [0.1954, 0.2077, 0.1918, 0.1935, 0.2116],\n",
            "        [0.1867, 0.2099, 0.1942, 0.1977, 0.2115],\n",
            "        [0.2111, 0.1997, 0.1885, 0.1944, 0.2063],\n",
            "        [0.2082, 0.1967, 0.1898, 0.1962, 0.2091],\n",
            "        [0.1809, 0.2189, 0.1903, 0.1971, 0.2129],\n",
            "        [0.2103, 0.2010, 0.1851, 0.1975, 0.2061]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 08:39:12 PM | epoch   2 |    50/  103 batches | lr 20.00 | ms/batch 3304.98 | loss  9.23 | ppl 10210.92\n",
            "04/28 08:41:54 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.1992, 0.1972, 0.2015, 0.2043, 0.1979],\n",
            "        [0.1914, 0.2180, 0.1958, 0.1974, 0.1975],\n",
            "        [0.2054, 0.2154, 0.1841, 0.2005, 0.1946],\n",
            "        [0.1836, 0.2209, 0.1942, 0.2099, 0.1915],\n",
            "        [0.2094, 0.2083, 0.1881, 0.2027, 0.1915],\n",
            "        [0.1997, 0.2193, 0.1876, 0.2022, 0.1912],\n",
            "        [0.1942, 0.1971, 0.2023, 0.2011, 0.2053],\n",
            "        [0.2152, 0.1932, 0.2063, 0.1994, 0.1859],\n",
            "        [0.2058, 0.1878, 0.2052, 0.1991, 0.2022],\n",
            "        [0.1808, 0.2059, 0.2097, 0.2102, 0.1935],\n",
            "        [0.1995, 0.1998, 0.1965, 0.2003, 0.2040],\n",
            "        [0.2028, 0.1964, 0.2010, 0.2020, 0.1978],\n",
            "        [0.2106, 0.1927, 0.1923, 0.1980, 0.2065],\n",
            "        [0.1816, 0.1936, 0.2091, 0.2045, 0.2112],\n",
            "        [0.2111, 0.1869, 0.1961, 0.2004, 0.2054],\n",
            "        [0.2069, 0.1926, 0.2006, 0.1891, 0.2107],\n",
            "        [0.2077, 0.1881, 0.2027, 0.1837, 0.2178],\n",
            "        [0.1918, 0.1888, 0.2121, 0.1845, 0.2228],\n",
            "        [0.1873, 0.1909, 0.2113, 0.1840, 0.2264],\n",
            "        [0.2156, 0.1849, 0.1981, 0.1860, 0.2154],\n",
            "        [0.2045, 0.1858, 0.2055, 0.1860, 0.2181],\n",
            "        [0.2021, 0.1987, 0.1960, 0.1937, 0.2095],\n",
            "        [0.2000, 0.1996, 0.1967, 0.1878, 0.2158],\n",
            "        [0.2123, 0.1901, 0.1970, 0.1951, 0.2056],\n",
            "        [0.1795, 0.2009, 0.2101, 0.1900, 0.2195],\n",
            "        [0.2197, 0.1978, 0.1888, 0.1912, 0.2025],\n",
            "        [0.1943, 0.2005, 0.1981, 0.1967, 0.2105],\n",
            "        [0.1934, 0.2049, 0.1992, 0.1980, 0.2045],\n",
            "        [0.1955, 0.2104, 0.1908, 0.1964, 0.2069],\n",
            "        [0.2100, 0.2120, 0.1860, 0.2006, 0.1914],\n",
            "        [0.1955, 0.2101, 0.1921, 0.1965, 0.2058],\n",
            "        [0.1824, 0.2161, 0.1956, 0.2000, 0.2058],\n",
            "        [0.2182, 0.2013, 0.1885, 0.1952, 0.1969],\n",
            "        [0.2043, 0.1997, 0.1930, 0.1992, 0.2039],\n",
            "        [0.1802, 0.2199, 0.1924, 0.2004, 0.2071],\n",
            "        [0.2040, 0.2071, 0.1873, 0.2000, 0.2015]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 08:41:54 PM | epoch   2 |   100/  103 batches | lr 20.00 | ms/batch 3248.23 | loss  8.75 | ppl  6306.47\n",
            "04/28 08:43:11 PM -----------------------------------------------------------------------------------------\n",
            "04/28 08:43:11 PM | end of epoch   2 | time: 405.13s | valid loss  8.02 | valid ppl  3047.12\n",
            "04/28 08:43:11 PM -----------------------------------------------------------------------------------------\n",
            "04/28 08:43:12 PM Saving Normal!\n",
            "04/28 08:45:57 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2070, 0.2004, 0.1938, 0.2096, 0.1892],\n",
            "        [0.1820, 0.2315, 0.1865, 0.1916, 0.2084],\n",
            "        [0.2147, 0.2220, 0.1723, 0.1901, 0.2010],\n",
            "        [0.1681, 0.2331, 0.1803, 0.2200, 0.1986],\n",
            "        [0.2174, 0.2146, 0.1727, 0.1947, 0.2005],\n",
            "        [0.2133, 0.2186, 0.1714, 0.1982, 0.1985],\n",
            "        [0.1821, 0.2030, 0.2091, 0.1939, 0.2120],\n",
            "        [0.2217, 0.1974, 0.1965, 0.2061, 0.1782],\n",
            "        [0.2153, 0.1807, 0.1970, 0.1934, 0.2136],\n",
            "        [0.1724, 0.2059, 0.2105, 0.2148, 0.1963],\n",
            "        [0.1877, 0.2076, 0.1871, 0.2092, 0.2084],\n",
            "        [0.2052, 0.1988, 0.1887, 0.2070, 0.2002],\n",
            "        [0.2140, 0.1945, 0.1787, 0.2008, 0.2119],\n",
            "        [0.1754, 0.2028, 0.2010, 0.1984, 0.2223],\n",
            "        [0.2098, 0.1828, 0.1915, 0.1989, 0.2170],\n",
            "        [0.2014, 0.1919, 0.1936, 0.1885, 0.2246],\n",
            "        [0.2174, 0.1842, 0.1937, 0.1792, 0.2255],\n",
            "        [0.2006, 0.1848, 0.2022, 0.1816, 0.2309],\n",
            "        [0.1852, 0.1884, 0.2052, 0.1824, 0.2387],\n",
            "        [0.2273, 0.1805, 0.1880, 0.1828, 0.2214],\n",
            "        [0.1999, 0.1867, 0.1999, 0.1867, 0.2267],\n",
            "        [0.1911, 0.2064, 0.1866, 0.1999, 0.2160],\n",
            "        [0.2082, 0.2113, 0.1895, 0.1833, 0.2077],\n",
            "        [0.2051, 0.2000, 0.1902, 0.1888, 0.2158],\n",
            "        [0.1723, 0.2075, 0.1993, 0.1958, 0.2250],\n",
            "        [0.2093, 0.2065, 0.1789, 0.1991, 0.2063],\n",
            "        [0.1838, 0.2085, 0.1872, 0.2047, 0.2159],\n",
            "        [0.2007, 0.2134, 0.1883, 0.1908, 0.2069],\n",
            "        [0.1860, 0.2206, 0.1866, 0.1909, 0.2158],\n",
            "        [0.2225, 0.2041, 0.1805, 0.1952, 0.1977],\n",
            "        [0.2040, 0.2021, 0.1879, 0.1913, 0.2146],\n",
            "        [0.1777, 0.2240, 0.1907, 0.1946, 0.2129],\n",
            "        [0.2318, 0.1923, 0.1827, 0.1905, 0.2026],\n",
            "        [0.1973, 0.2083, 0.1897, 0.1948, 0.2099],\n",
            "        [0.1862, 0.2255, 0.1850, 0.1936, 0.2096],\n",
            "        [0.1902, 0.2150, 0.1809, 0.2081, 0.2058]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 08:45:57 PM | epoch   3 |    50/  103 batches | lr 20.00 | ms/batch 3315.03 | loss  8.82 | ppl  6786.25\n",
            "04/28 08:48:48 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2077, 0.2012, 0.1926, 0.2104, 0.1881],\n",
            "        [0.1812, 0.2327, 0.1857, 0.1907, 0.2097],\n",
            "        [0.2156, 0.2227, 0.1712, 0.1888, 0.2018],\n",
            "        [0.1669, 0.2338, 0.1791, 0.2208, 0.1993],\n",
            "        [0.2182, 0.2153, 0.1717, 0.1935, 0.2013],\n",
            "        [0.2146, 0.2176, 0.1707, 0.1974, 0.1997],\n",
            "        [0.1809, 0.2037, 0.2100, 0.1925, 0.2128],\n",
            "        [0.2226, 0.1981, 0.1952, 0.2072, 0.1770],\n",
            "        [0.2166, 0.1799, 0.1961, 0.1926, 0.2149],\n",
            "        [0.1708, 0.2060, 0.2115, 0.2151, 0.1966],\n",
            "        [0.1865, 0.2084, 0.1859, 0.2101, 0.2092],\n",
            "        [0.2056, 0.1992, 0.1871, 0.2076, 0.2005],\n",
            "        [0.2144, 0.1949, 0.1772, 0.2013, 0.2122],\n",
            "        [0.1745, 0.2039, 0.2000, 0.1981, 0.2234],\n",
            "        [0.2088, 0.1840, 0.1907, 0.1981, 0.2184],\n",
            "        [0.2009, 0.1915, 0.1932, 0.1881, 0.2264],\n",
            "        [0.2186, 0.1834, 0.1928, 0.1784, 0.2268],\n",
            "        [0.2017, 0.1839, 0.2013, 0.1808, 0.2322],\n",
            "        [0.1847, 0.1880, 0.2047, 0.1820, 0.2406],\n",
            "        [0.2286, 0.1797, 0.1871, 0.1820, 0.2226],\n",
            "        [0.1995, 0.1863, 0.1995, 0.1863, 0.2285],\n",
            "        [0.1899, 0.2072, 0.1854, 0.2008, 0.2168],\n",
            "        [0.2095, 0.2125, 0.1887, 0.1826, 0.2067],\n",
            "        [0.2043, 0.2012, 0.1894, 0.1880, 0.2171],\n",
            "        [0.1712, 0.2082, 0.1980, 0.1967, 0.2258],\n",
            "        [0.2080, 0.2073, 0.1778, 0.1999, 0.2070],\n",
            "        [0.1825, 0.2092, 0.1860, 0.2057, 0.2166],\n",
            "        [0.2015, 0.2141, 0.1871, 0.1896, 0.2077],\n",
            "        [0.1852, 0.2219, 0.1858, 0.1901, 0.2171],\n",
            "        [0.2239, 0.2032, 0.1797, 0.1944, 0.1988],\n",
            "        [0.2052, 0.2012, 0.1871, 0.1905, 0.2159],\n",
            "        [0.1769, 0.2253, 0.1899, 0.1938, 0.2141],\n",
            "        [0.2332, 0.1915, 0.1819, 0.1897, 0.2037],\n",
            "        [0.1965, 0.2095, 0.1889, 0.1940, 0.2111],\n",
            "        [0.1870, 0.2264, 0.1838, 0.1924, 0.2104],\n",
            "        [0.1890, 0.2157, 0.1798, 0.2090, 0.2065]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 08:48:48 PM | epoch   3 |   100/  103 batches | lr 20.00 | ms/batch 3407.60 | loss  8.52 | ppl  5004.22\n",
            "04/28 08:50:04 PM -----------------------------------------------------------------------------------------\n",
            "04/28 08:50:04 PM | end of epoch   3 | time: 412.10s | valid loss  7.53 | valid ppl  1857.46\n",
            "04/28 08:50:04 PM -----------------------------------------------------------------------------------------\n",
            "04/28 08:50:04 PM Saving Normal!\n",
            "04/28 08:52:57 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('identity', 2), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2077, 0.2012, 0.1926, 0.2104, 0.1881],\n",
            "        [0.1812, 0.2327, 0.1857, 0.1907, 0.2097],\n",
            "        [0.2156, 0.2227, 0.1712, 0.1887, 0.2018],\n",
            "        [0.1669, 0.2338, 0.1791, 0.2208, 0.1994],\n",
            "        [0.2183, 0.2152, 0.1717, 0.1935, 0.2013],\n",
            "        [0.2147, 0.2176, 0.1707, 0.1974, 0.1997],\n",
            "        [0.1809, 0.2036, 0.2102, 0.1925, 0.2128],\n",
            "        [0.2225, 0.1980, 0.1951, 0.2075, 0.1768],\n",
            "        [0.2166, 0.1798, 0.1961, 0.1925, 0.2151],\n",
            "        [0.1707, 0.2059, 0.2119, 0.2150, 0.1965],\n",
            "        [0.1865, 0.2084, 0.1858, 0.2101, 0.2092],\n",
            "        [0.2057, 0.1991, 0.1871, 0.2076, 0.2005],\n",
            "        [0.2144, 0.1948, 0.1772, 0.2013, 0.2122],\n",
            "        [0.1745, 0.2039, 0.1999, 0.1984, 0.2233],\n",
            "        [0.2086, 0.1845, 0.1906, 0.1980, 0.2183],\n",
            "        [0.2009, 0.1915, 0.1932, 0.1881, 0.2264],\n",
            "        [0.2186, 0.1834, 0.1928, 0.1784, 0.2267],\n",
            "        [0.2017, 0.1839, 0.2013, 0.1808, 0.2322],\n",
            "        [0.1847, 0.1880, 0.2047, 0.1820, 0.2406],\n",
            "        [0.2286, 0.1797, 0.1871, 0.1820, 0.2226],\n",
            "        [0.1995, 0.1863, 0.1995, 0.1863, 0.2285],\n",
            "        [0.1898, 0.2072, 0.1853, 0.2008, 0.2168],\n",
            "        [0.2095, 0.2125, 0.1887, 0.1826, 0.2068],\n",
            "        [0.2043, 0.2012, 0.1894, 0.1880, 0.2171],\n",
            "        [0.1712, 0.2082, 0.1980, 0.1967, 0.2259],\n",
            "        [0.2080, 0.2073, 0.1777, 0.1999, 0.2071],\n",
            "        [0.1826, 0.2092, 0.1860, 0.2056, 0.2166],\n",
            "        [0.1997, 0.2121, 0.1854, 0.1878, 0.2150],\n",
            "        [0.1851, 0.2219, 0.1858, 0.1901, 0.2171],\n",
            "        [0.2239, 0.2031, 0.1797, 0.1944, 0.1988],\n",
            "        [0.2052, 0.2012, 0.1871, 0.1904, 0.2160],\n",
            "        [0.1768, 0.2253, 0.1899, 0.1938, 0.2141],\n",
            "        [0.2332, 0.1914, 0.1819, 0.1897, 0.2038],\n",
            "        [0.1965, 0.2095, 0.1889, 0.1940, 0.2111],\n",
            "        [0.1870, 0.2264, 0.1838, 0.1924, 0.2104],\n",
            "        [0.1890, 0.2157, 0.1798, 0.2090, 0.2066]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 08:52:57 PM | epoch   4 |    50/  103 batches | lr 20.00 | ms/batch 3466.47 | loss  8.62 | ppl  5518.82\n",
            "04/28 08:55:49 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('identity', 2), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2076, 0.2012, 0.1926, 0.2104, 0.1882],\n",
            "        [0.1812, 0.2327, 0.1857, 0.1907, 0.2097],\n",
            "        [0.2156, 0.2227, 0.1712, 0.1887, 0.2018],\n",
            "        [0.1669, 0.2338, 0.1791, 0.2208, 0.1993],\n",
            "        [0.2183, 0.2152, 0.1717, 0.1935, 0.2013],\n",
            "        [0.2146, 0.2176, 0.1707, 0.1974, 0.1997],\n",
            "        [0.1809, 0.2036, 0.2102, 0.1925, 0.2128],\n",
            "        [0.2226, 0.1981, 0.1952, 0.2071, 0.1769],\n",
            "        [0.2165, 0.1798, 0.1961, 0.1925, 0.2152],\n",
            "        [0.1707, 0.2059, 0.2119, 0.2149, 0.1965],\n",
            "        [0.1865, 0.2084, 0.1859, 0.2101, 0.2092],\n",
            "        [0.2057, 0.1990, 0.1871, 0.2076, 0.2006],\n",
            "        [0.2144, 0.1948, 0.1772, 0.2012, 0.2123],\n",
            "        [0.1745, 0.2039, 0.2000, 0.1983, 0.2234],\n",
            "        [0.2086, 0.1843, 0.1907, 0.1981, 0.2184],\n",
            "        [0.2009, 0.1915, 0.1932, 0.1881, 0.2264],\n",
            "        [0.2186, 0.1834, 0.1928, 0.1784, 0.2267],\n",
            "        [0.2017, 0.1839, 0.2013, 0.1808, 0.2322],\n",
            "        [0.1847, 0.1880, 0.2047, 0.1820, 0.2406],\n",
            "        [0.2286, 0.1797, 0.1871, 0.1820, 0.2226],\n",
            "        [0.1995, 0.1862, 0.1995, 0.1863, 0.2285],\n",
            "        [0.1899, 0.2072, 0.1854, 0.2007, 0.2169],\n",
            "        [0.2095, 0.2124, 0.1887, 0.1826, 0.2068],\n",
            "        [0.2043, 0.2012, 0.1894, 0.1880, 0.2171],\n",
            "        [0.1712, 0.2082, 0.1980, 0.1967, 0.2259],\n",
            "        [0.2080, 0.2073, 0.1777, 0.1999, 0.2071],\n",
            "        [0.1826, 0.2092, 0.1860, 0.2055, 0.2167],\n",
            "        [0.1987, 0.2110, 0.1846, 0.1869, 0.2188],\n",
            "        [0.1851, 0.2219, 0.1858, 0.1901, 0.2171],\n",
            "        [0.2239, 0.2031, 0.1797, 0.1944, 0.1988],\n",
            "        [0.2052, 0.2012, 0.1871, 0.1904, 0.2161],\n",
            "        [0.1769, 0.2253, 0.1899, 0.1938, 0.2141],\n",
            "        [0.2332, 0.1914, 0.1819, 0.1897, 0.2038],\n",
            "        [0.1965, 0.2095, 0.1889, 0.1940, 0.2111],\n",
            "        [0.1870, 0.2264, 0.1838, 0.1924, 0.2104],\n",
            "        [0.1890, 0.2157, 0.1798, 0.2090, 0.2066]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 08:55:49 PM | epoch   4 |   100/  103 batches | lr 20.00 | ms/batch 3429.67 | loss  8.20 | ppl  3623.05\n",
            "04/28 08:57:09 PM -----------------------------------------------------------------------------------------\n",
            "04/28 08:57:09 PM | end of epoch   4 | time: 424.85s | valid loss  7.24 | valid ppl  1387.62\n",
            "04/28 08:57:09 PM -----------------------------------------------------------------------------------------\n",
            "04/28 08:57:09 PM Saving Normal!\n",
            "04/28 09:00:01 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('identity', 2), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2075, 0.2013, 0.1927, 0.2105, 0.1881],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2097],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1887, 0.2017],\n",
            "        [0.1670, 0.2338, 0.1792, 0.2207, 0.1994],\n",
            "        [0.2184, 0.2152, 0.1717, 0.1934, 0.2013],\n",
            "        [0.2146, 0.2176, 0.1707, 0.1973, 0.1997],\n",
            "        [0.1809, 0.2036, 0.2103, 0.1924, 0.2128],\n",
            "        [0.2229, 0.1982, 0.1954, 0.2065, 0.1770],\n",
            "        [0.2166, 0.1798, 0.1961, 0.1925, 0.2149],\n",
            "        [0.1707, 0.2058, 0.2124, 0.2147, 0.1965],\n",
            "        [0.1865, 0.2085, 0.1859, 0.2098, 0.2093],\n",
            "        [0.2058, 0.1989, 0.1871, 0.2075, 0.2006],\n",
            "        [0.2145, 0.1949, 0.1773, 0.2007, 0.2127],\n",
            "        [0.1746, 0.2040, 0.2001, 0.1979, 0.2235],\n",
            "        [0.2086, 0.1840, 0.1907, 0.1982, 0.2185],\n",
            "        [0.2010, 0.1914, 0.1931, 0.1881, 0.2264],\n",
            "        [0.2186, 0.1834, 0.1928, 0.1784, 0.2268],\n",
            "        [0.2017, 0.1839, 0.2013, 0.1808, 0.2323],\n",
            "        [0.1847, 0.1880, 0.2047, 0.1820, 0.2406],\n",
            "        [0.2286, 0.1797, 0.1871, 0.1820, 0.2227],\n",
            "        [0.1995, 0.1862, 0.1995, 0.1862, 0.2285],\n",
            "        [0.1899, 0.2072, 0.1854, 0.2007, 0.2169],\n",
            "        [0.2095, 0.2125, 0.1887, 0.1826, 0.2068],\n",
            "        [0.2042, 0.2012, 0.1894, 0.1880, 0.2171],\n",
            "        [0.1712, 0.2082, 0.1980, 0.1967, 0.2259],\n",
            "        [0.2079, 0.2073, 0.1778, 0.2000, 0.2071],\n",
            "        [0.1826, 0.2092, 0.1860, 0.2057, 0.2166],\n",
            "        [0.2005, 0.2130, 0.1862, 0.1886, 0.2117],\n",
            "        [0.1852, 0.2219, 0.1858, 0.1900, 0.2170],\n",
            "        [0.2239, 0.2032, 0.1797, 0.1944, 0.1988],\n",
            "        [0.2055, 0.2015, 0.1874, 0.1907, 0.2149],\n",
            "        [0.1768, 0.2254, 0.1899, 0.1938, 0.2141],\n",
            "        [0.2331, 0.1916, 0.1819, 0.1897, 0.2037],\n",
            "        [0.1965, 0.2095, 0.1889, 0.1940, 0.2111],\n",
            "        [0.1870, 0.2264, 0.1838, 0.1924, 0.2104],\n",
            "        [0.1890, 0.2157, 0.1797, 0.2090, 0.2065]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:00:01 PM | epoch   5 |    50/  103 batches | lr 20.00 | ms/batch 3440.65 | loss  8.29 | ppl  3975.73\n",
            "04/28 09:02:53 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('identity', 2), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2074, 0.2013, 0.1927, 0.2105, 0.1881],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2097],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1887, 0.2017],\n",
            "        [0.1670, 0.2338, 0.1792, 0.2207, 0.1993],\n",
            "        [0.2184, 0.2152, 0.1717, 0.1934, 0.2013],\n",
            "        [0.2146, 0.2176, 0.1707, 0.1973, 0.1997],\n",
            "        [0.1809, 0.2036, 0.2103, 0.1924, 0.2128],\n",
            "        [0.2229, 0.1983, 0.1954, 0.2063, 0.1770],\n",
            "        [0.2166, 0.1798, 0.1961, 0.1925, 0.2150],\n",
            "        [0.1707, 0.2058, 0.2124, 0.2147, 0.1965],\n",
            "        [0.1865, 0.2085, 0.1859, 0.2098, 0.2093],\n",
            "        [0.2058, 0.1989, 0.1871, 0.2074, 0.2007],\n",
            "        [0.2145, 0.1949, 0.1773, 0.2008, 0.2126],\n",
            "        [0.1746, 0.2040, 0.2001, 0.1978, 0.2235],\n",
            "        [0.2084, 0.1842, 0.1907, 0.1982, 0.2185],\n",
            "        [0.2010, 0.1915, 0.1931, 0.1881, 0.2264],\n",
            "        [0.2186, 0.1834, 0.1928, 0.1784, 0.2268],\n",
            "        [0.2017, 0.1839, 0.2013, 0.1808, 0.2323],\n",
            "        [0.1847, 0.1880, 0.2047, 0.1820, 0.2406],\n",
            "        [0.2286, 0.1796, 0.1871, 0.1820, 0.2227],\n",
            "        [0.1995, 0.1862, 0.1995, 0.1862, 0.2285],\n",
            "        [0.1899, 0.2072, 0.1854, 0.2007, 0.2169],\n",
            "        [0.2095, 0.2125, 0.1887, 0.1826, 0.2067],\n",
            "        [0.2042, 0.2012, 0.1894, 0.1880, 0.2172],\n",
            "        [0.1712, 0.2082, 0.1980, 0.1967, 0.2259],\n",
            "        [0.2079, 0.2073, 0.1778, 0.2000, 0.2071],\n",
            "        [0.1825, 0.2092, 0.1859, 0.2058, 0.2166],\n",
            "        [0.2006, 0.2131, 0.1863, 0.1887, 0.2112],\n",
            "        [0.1852, 0.2219, 0.1858, 0.1900, 0.2170],\n",
            "        [0.2239, 0.2031, 0.1797, 0.1944, 0.1988],\n",
            "        [0.2056, 0.2016, 0.1875, 0.1908, 0.2145],\n",
            "        [0.1768, 0.2253, 0.1899, 0.1938, 0.2141],\n",
            "        [0.2331, 0.1916, 0.1819, 0.1897, 0.2038],\n",
            "        [0.1965, 0.2095, 0.1889, 0.1939, 0.2111],\n",
            "        [0.1870, 0.2264, 0.1838, 0.1924, 0.2104],\n",
            "        [0.1890, 0.2157, 0.1797, 0.2090, 0.2065]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:02:53 PM | epoch   5 |   100/  103 batches | lr 20.00 | ms/batch 3435.87 | loss  8.11 | ppl  3332.83\n",
            "04/28 09:04:08 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:04:08 PM | end of epoch   5 | time: 419.20s | valid loss  7.11 | valid ppl  1222.94\n",
            "04/28 09:04:08 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:04:08 PM Saving Normal!\n",
            "04/28 09:06:55 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('identity', 2), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2073, 0.2013, 0.1927, 0.2105, 0.1881],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2097],\n",
            "        [0.2156, 0.2227, 0.1712, 0.1887, 0.2017],\n",
            "        [0.1669, 0.2338, 0.1792, 0.2207, 0.1994],\n",
            "        [0.2185, 0.2151, 0.1717, 0.1933, 0.2013],\n",
            "        [0.2146, 0.2176, 0.1707, 0.1973, 0.1997],\n",
            "        [0.1809, 0.2036, 0.2103, 0.1923, 0.2128],\n",
            "        [0.2230, 0.1983, 0.1956, 0.2060, 0.1772],\n",
            "        [0.2165, 0.1797, 0.1961, 0.1924, 0.2153],\n",
            "        [0.1707, 0.2058, 0.2124, 0.2145, 0.1966],\n",
            "        [0.1866, 0.2085, 0.1859, 0.2097, 0.2093],\n",
            "        [0.2059, 0.1987, 0.1872, 0.2074, 0.2008],\n",
            "        [0.2146, 0.1948, 0.1773, 0.2004, 0.2128],\n",
            "        [0.1746, 0.2040, 0.2001, 0.1977, 0.2236],\n",
            "        [0.2086, 0.1833, 0.1910, 0.1984, 0.2187],\n",
            "        [0.2009, 0.1914, 0.1931, 0.1881, 0.2264],\n",
            "        [0.2186, 0.1834, 0.1928, 0.1784, 0.2268],\n",
            "        [0.2017, 0.1839, 0.2013, 0.1808, 0.2323],\n",
            "        [0.1847, 0.1880, 0.2047, 0.1820, 0.2406],\n",
            "        [0.2286, 0.1796, 0.1871, 0.1820, 0.2227],\n",
            "        [0.1995, 0.1862, 0.1995, 0.1862, 0.2285],\n",
            "        [0.1899, 0.2072, 0.1854, 0.2006, 0.2169],\n",
            "        [0.2095, 0.2125, 0.1887, 0.1826, 0.2067],\n",
            "        [0.2044, 0.2012, 0.1894, 0.1880, 0.2171],\n",
            "        [0.1712, 0.2082, 0.1981, 0.1965, 0.2259],\n",
            "        [0.2078, 0.2073, 0.1778, 0.1999, 0.2073],\n",
            "        [0.1826, 0.2092, 0.1860, 0.2056, 0.2166],\n",
            "        [0.2006, 0.2133, 0.1864, 0.1888, 0.2109],\n",
            "        [0.1852, 0.2219, 0.1858, 0.1901, 0.2170],\n",
            "        [0.2240, 0.2030, 0.1797, 0.1944, 0.1989],\n",
            "        [0.2056, 0.2016, 0.1874, 0.1908, 0.2147],\n",
            "        [0.1768, 0.2254, 0.1899, 0.1938, 0.2141],\n",
            "        [0.2331, 0.1915, 0.1819, 0.1897, 0.2038],\n",
            "        [0.1965, 0.2095, 0.1889, 0.1939, 0.2111],\n",
            "        [0.1870, 0.2264, 0.1838, 0.1924, 0.2104],\n",
            "        [0.1890, 0.2157, 0.1797, 0.2091, 0.2065]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:06:55 PM | epoch   6 |    50/  103 batches | lr 20.00 | ms/batch 3338.45 | loss  8.21 | ppl  3666.26\n",
            "04/28 09:09:45 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('identity', 2), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2073, 0.2014, 0.1927, 0.2105, 0.1881],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2097],\n",
            "        [0.2156, 0.2227, 0.1712, 0.1887, 0.2017],\n",
            "        [0.1669, 0.2338, 0.1792, 0.2207, 0.1993],\n",
            "        [0.2186, 0.2152, 0.1717, 0.1933, 0.2013],\n",
            "        [0.2146, 0.2176, 0.1707, 0.1973, 0.1997],\n",
            "        [0.1810, 0.2035, 0.2104, 0.1923, 0.2129],\n",
            "        [0.2232, 0.1984, 0.1957, 0.2054, 0.1773],\n",
            "        [0.2165, 0.1797, 0.1960, 0.1924, 0.2155],\n",
            "        [0.1707, 0.2058, 0.2125, 0.2144, 0.1966],\n",
            "        [0.1865, 0.2085, 0.1859, 0.2098, 0.2093],\n",
            "        [0.2059, 0.1987, 0.1872, 0.2074, 0.2008],\n",
            "        [0.2146, 0.1949, 0.1773, 0.2005, 0.2127],\n",
            "        [0.1746, 0.2040, 0.2001, 0.1977, 0.2236],\n",
            "        [0.2086, 0.1835, 0.1909, 0.1983, 0.2187],\n",
            "        [0.2010, 0.1914, 0.1931, 0.1881, 0.2264],\n",
            "        [0.2186, 0.1833, 0.1928, 0.1784, 0.2268],\n",
            "        [0.2017, 0.1839, 0.2013, 0.1808, 0.2323],\n",
            "        [0.1847, 0.1879, 0.2047, 0.1820, 0.2406],\n",
            "        [0.2286, 0.1796, 0.1871, 0.1820, 0.2227],\n",
            "        [0.1995, 0.1862, 0.1995, 0.1862, 0.2286],\n",
            "        [0.1899, 0.2072, 0.1854, 0.2006, 0.2169],\n",
            "        [0.2095, 0.2124, 0.1887, 0.1826, 0.2067],\n",
            "        [0.2045, 0.2011, 0.1894, 0.1879, 0.2171],\n",
            "        [0.1712, 0.2082, 0.1981, 0.1965, 0.2260],\n",
            "        [0.2077, 0.2073, 0.1778, 0.1999, 0.2073],\n",
            "        [0.1826, 0.2092, 0.1860, 0.2055, 0.2167],\n",
            "        [0.2005, 0.2131, 0.1863, 0.1887, 0.2114],\n",
            "        [0.1852, 0.2219, 0.1858, 0.1901, 0.2170],\n",
            "        [0.2240, 0.2030, 0.1797, 0.1944, 0.1989],\n",
            "        [0.2058, 0.2018, 0.1876, 0.1910, 0.2138],\n",
            "        [0.1768, 0.2253, 0.1899, 0.1938, 0.2141],\n",
            "        [0.2330, 0.1916, 0.1819, 0.1897, 0.2038],\n",
            "        [0.1965, 0.2095, 0.1890, 0.1939, 0.2111],\n",
            "        [0.1869, 0.2265, 0.1838, 0.1924, 0.2104],\n",
            "        [0.1890, 0.2157, 0.1798, 0.2089, 0.2066]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:09:45 PM | epoch   6 |   100/  103 batches | lr 20.00 | ms/batch 3407.40 | loss  7.92 | ppl  2744.82\n",
            "04/28 09:11:07 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:11:07 PM | end of epoch   6 | time: 419.43s | valid loss  6.98 | valid ppl  1080.24\n",
            "04/28 09:11:07 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:11:08 PM Saving Normal!\n",
            "04/28 09:14:00 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('identity', 2), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2071, 0.2014, 0.1928, 0.2106, 0.1881],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2156, 0.2227, 0.1712, 0.1887, 0.2017],\n",
            "        [0.1669, 0.2338, 0.1792, 0.2207, 0.1994],\n",
            "        [0.2186, 0.2151, 0.1717, 0.1933, 0.2013],\n",
            "        [0.2146, 0.2176, 0.1707, 0.1973, 0.1997],\n",
            "        [0.1810, 0.2035, 0.2104, 0.1923, 0.2129],\n",
            "        [0.2233, 0.1985, 0.1958, 0.2049, 0.1775],\n",
            "        [0.2165, 0.1796, 0.1960, 0.1923, 0.2156],\n",
            "        [0.1707, 0.2057, 0.2126, 0.2143, 0.1966],\n",
            "        [0.1865, 0.2085, 0.1860, 0.2097, 0.2093],\n",
            "        [0.2060, 0.1986, 0.1873, 0.2073, 0.2008],\n",
            "        [0.2146, 0.1948, 0.1774, 0.2004, 0.2128],\n",
            "        [0.1746, 0.2040, 0.2001, 0.1977, 0.2236],\n",
            "        [0.2091, 0.1820, 0.1912, 0.1987, 0.2191],\n",
            "        [0.2009, 0.1914, 0.1932, 0.1881, 0.2264],\n",
            "        [0.2186, 0.1833, 0.1928, 0.1784, 0.2268],\n",
            "        [0.2017, 0.1839, 0.2013, 0.1808, 0.2323],\n",
            "        [0.1848, 0.1879, 0.2047, 0.1820, 0.2406],\n",
            "        [0.2285, 0.1796, 0.1871, 0.1820, 0.2227],\n",
            "        [0.1995, 0.1862, 0.1995, 0.1862, 0.2286],\n",
            "        [0.1899, 0.2072, 0.1854, 0.2006, 0.2169],\n",
            "        [0.2095, 0.2124, 0.1887, 0.1825, 0.2068],\n",
            "        [0.2044, 0.2012, 0.1894, 0.1879, 0.2171],\n",
            "        [0.1712, 0.2082, 0.1981, 0.1964, 0.2260],\n",
            "        [0.2078, 0.2073, 0.1778, 0.1999, 0.2072],\n",
            "        [0.1826, 0.2092, 0.1860, 0.2054, 0.2167],\n",
            "        [0.2008, 0.2134, 0.1865, 0.1889, 0.2103],\n",
            "        [0.1851, 0.2220, 0.1858, 0.1901, 0.2170],\n",
            "        [0.2240, 0.2032, 0.1797, 0.1944, 0.1987],\n",
            "        [0.2068, 0.2028, 0.1886, 0.1920, 0.2098],\n",
            "        [0.1768, 0.2254, 0.1899, 0.1939, 0.2141],\n",
            "        [0.2329, 0.1919, 0.1819, 0.1897, 0.2036],\n",
            "        [0.1964, 0.2095, 0.1890, 0.1940, 0.2111],\n",
            "        [0.1869, 0.2265, 0.1838, 0.1925, 0.2103],\n",
            "        [0.1891, 0.2157, 0.1798, 0.2089, 0.2066]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:14:00 PM | epoch   7 |    50/  103 batches | lr 20.00 | ms/batch 3456.29 | loss  8.07 | ppl  3204.15\n",
            "04/28 09:16:51 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('identity', 2), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2070, 0.2014, 0.1928, 0.2106, 0.1882],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2097],\n",
            "        [0.2156, 0.2227, 0.1712, 0.1888, 0.2017],\n",
            "        [0.1669, 0.2338, 0.1792, 0.2207, 0.1994],\n",
            "        [0.2187, 0.2151, 0.1717, 0.1933, 0.2013],\n",
            "        [0.2146, 0.2176, 0.1707, 0.1973, 0.1997],\n",
            "        [0.1810, 0.2035, 0.2103, 0.1923, 0.2129],\n",
            "        [0.2235, 0.1986, 0.1959, 0.2043, 0.1776],\n",
            "        [0.2164, 0.1796, 0.1960, 0.1923, 0.2158],\n",
            "        [0.1708, 0.2058, 0.2122, 0.2144, 0.1968],\n",
            "        [0.1865, 0.2085, 0.1860, 0.2097, 0.2093],\n",
            "        [0.2061, 0.1984, 0.1873, 0.2073, 0.2009],\n",
            "        [0.2146, 0.1948, 0.1774, 0.2003, 0.2128],\n",
            "        [0.1746, 0.2040, 0.2002, 0.1976, 0.2236],\n",
            "        [0.2091, 0.1815, 0.1914, 0.1988, 0.2192],\n",
            "        [0.2009, 0.1914, 0.1932, 0.1881, 0.2265],\n",
            "        [0.2186, 0.1833, 0.1928, 0.1784, 0.2269],\n",
            "        [0.2017, 0.1839, 0.2013, 0.1808, 0.2323],\n",
            "        [0.1847, 0.1879, 0.2047, 0.1820, 0.2406],\n",
            "        [0.2286, 0.1796, 0.1871, 0.1820, 0.2227],\n",
            "        [0.1995, 0.1862, 0.1995, 0.1862, 0.2286],\n",
            "        [0.1899, 0.2072, 0.1854, 0.2005, 0.2169],\n",
            "        [0.2095, 0.2124, 0.1887, 0.1825, 0.2068],\n",
            "        [0.2044, 0.2012, 0.1894, 0.1879, 0.2171],\n",
            "        [0.1712, 0.2082, 0.1981, 0.1964, 0.2260],\n",
            "        [0.2078, 0.2073, 0.1778, 0.1999, 0.2072],\n",
            "        [0.1826, 0.2092, 0.1860, 0.2054, 0.2167],\n",
            "        [0.2002, 0.2128, 0.1860, 0.1884, 0.2126],\n",
            "        [0.1851, 0.2220, 0.1858, 0.1901, 0.2170],\n",
            "        [0.2240, 0.2032, 0.1797, 0.1944, 0.1987],\n",
            "        [0.2069, 0.2030, 0.1887, 0.1921, 0.2093],\n",
            "        [0.1768, 0.2254, 0.1899, 0.1938, 0.2141],\n",
            "        [0.2330, 0.1918, 0.1819, 0.1897, 0.2036],\n",
            "        [0.1964, 0.2095, 0.1890, 0.1940, 0.2111],\n",
            "        [0.1870, 0.2265, 0.1838, 0.1925, 0.2102],\n",
            "        [0.1890, 0.2157, 0.1798, 0.2090, 0.2065]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:16:51 PM | epoch   7 |   100/  103 batches | lr 20.00 | ms/batch 3417.49 | loss  7.78 | ppl  2388.86\n",
            "04/28 09:18:09 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:18:09 PM | end of epoch   7 | time: 421.28s | valid loss  6.93 | valid ppl  1025.55\n",
            "04/28 09:18:09 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:18:09 PM Saving Normal!\n",
            "04/28 09:21:03 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('identity', 2), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2069, 0.2015, 0.1928, 0.2106, 0.1882],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1888, 0.2017],\n",
            "        [0.1669, 0.2338, 0.1792, 0.2207, 0.1993],\n",
            "        [0.2186, 0.2151, 0.1717, 0.1933, 0.2013],\n",
            "        [0.2146, 0.2176, 0.1707, 0.1973, 0.1997],\n",
            "        [0.1810, 0.2035, 0.2103, 0.1923, 0.2129],\n",
            "        [0.2235, 0.1986, 0.1959, 0.2043, 0.1777],\n",
            "        [0.2164, 0.1796, 0.1959, 0.1923, 0.2158],\n",
            "        [0.1708, 0.2059, 0.2119, 0.2145, 0.1968],\n",
            "        [0.1865, 0.2085, 0.1860, 0.2097, 0.2093],\n",
            "        [0.2060, 0.1985, 0.1873, 0.2073, 0.2008],\n",
            "        [0.2146, 0.1949, 0.1774, 0.2004, 0.2127],\n",
            "        [0.1746, 0.2040, 0.2001, 0.1977, 0.2235],\n",
            "        [0.2092, 0.1812, 0.1914, 0.1988, 0.2193],\n",
            "        [0.2009, 0.1914, 0.1932, 0.1881, 0.2264],\n",
            "        [0.2186, 0.1833, 0.1928, 0.1784, 0.2269],\n",
            "        [0.2018, 0.1839, 0.2013, 0.1808, 0.2323],\n",
            "        [0.1847, 0.1879, 0.2047, 0.1820, 0.2406],\n",
            "        [0.2285, 0.1796, 0.1872, 0.1820, 0.2227],\n",
            "        [0.1995, 0.1862, 0.1995, 0.1862, 0.2286],\n",
            "        [0.1899, 0.2072, 0.1854, 0.2005, 0.2169],\n",
            "        [0.2096, 0.2124, 0.1888, 0.1825, 0.2068],\n",
            "        [0.2046, 0.2011, 0.1893, 0.1879, 0.2170],\n",
            "        [0.1712, 0.2082, 0.1982, 0.1963, 0.2260],\n",
            "        [0.2078, 0.2073, 0.1778, 0.1999, 0.2072],\n",
            "        [0.1826, 0.2092, 0.1860, 0.2056, 0.2166],\n",
            "        [0.1992, 0.2117, 0.1851, 0.1875, 0.2165],\n",
            "        [0.1851, 0.2220, 0.1858, 0.1901, 0.2170],\n",
            "        [0.2240, 0.2031, 0.1798, 0.1944, 0.1987],\n",
            "        [0.2069, 0.2029, 0.1887, 0.1921, 0.2094],\n",
            "        [0.1768, 0.2254, 0.1899, 0.1939, 0.2141],\n",
            "        [0.2330, 0.1918, 0.1819, 0.1897, 0.2036],\n",
            "        [0.1964, 0.2095, 0.1890, 0.1940, 0.2111],\n",
            "        [0.1869, 0.2266, 0.1838, 0.1925, 0.2102],\n",
            "        [0.1889, 0.2157, 0.1797, 0.2091, 0.2065]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:21:03 PM | epoch   8 |    50/  103 batches | lr 20.00 | ms/batch 3473.62 | loss  7.91 | ppl  2719.06\n",
            "04/28 09:23:48 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('identity', 2), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2068, 0.2015, 0.1928, 0.2107, 0.1882],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1888, 0.2017],\n",
            "        [0.1669, 0.2338, 0.1792, 0.2207, 0.1993],\n",
            "        [0.2187, 0.2151, 0.1717, 0.1933, 0.2013],\n",
            "        [0.2146, 0.2176, 0.1707, 0.1973, 0.1997],\n",
            "        [0.1809, 0.2035, 0.2104, 0.1923, 0.2128],\n",
            "        [0.2232, 0.1985, 0.1958, 0.2050, 0.1775],\n",
            "        [0.2166, 0.1797, 0.1961, 0.1924, 0.2151],\n",
            "        [0.1707, 0.2058, 0.2123, 0.2146, 0.1966],\n",
            "        [0.1865, 0.2085, 0.1860, 0.2098, 0.2093],\n",
            "        [0.2060, 0.1986, 0.1873, 0.2073, 0.2008],\n",
            "        [0.2146, 0.1949, 0.1774, 0.2005, 0.2127],\n",
            "        [0.1746, 0.2040, 0.2001, 0.1978, 0.2235],\n",
            "        [0.2096, 0.1803, 0.1916, 0.1990, 0.2195],\n",
            "        [0.2010, 0.1914, 0.1932, 0.1881, 0.2265],\n",
            "        [0.2187, 0.1833, 0.1928, 0.1784, 0.2269],\n",
            "        [0.2018, 0.1838, 0.2013, 0.1807, 0.2323],\n",
            "        [0.1848, 0.1879, 0.2047, 0.1820, 0.2406],\n",
            "        [0.2282, 0.1797, 0.1872, 0.1820, 0.2229],\n",
            "        [0.1994, 0.1862, 0.1995, 0.1862, 0.2286],\n",
            "        [0.1899, 0.2072, 0.1854, 0.2006, 0.2169],\n",
            "        [0.2096, 0.2124, 0.1888, 0.1825, 0.2068],\n",
            "        [0.2049, 0.2010, 0.1893, 0.1878, 0.2169],\n",
            "        [0.1713, 0.2083, 0.1982, 0.1963, 0.2260],\n",
            "        [0.2077, 0.2073, 0.1778, 0.2000, 0.2072],\n",
            "        [0.1824, 0.2091, 0.1859, 0.2060, 0.2165],\n",
            "        [0.1991, 0.2116, 0.1850, 0.1874, 0.2169],\n",
            "        [0.1851, 0.2219, 0.1859, 0.1901, 0.2170],\n",
            "        [0.2240, 0.2031, 0.1798, 0.1944, 0.1987],\n",
            "        [0.2071, 0.2031, 0.1889, 0.1922, 0.2087],\n",
            "        [0.1768, 0.2253, 0.1900, 0.1938, 0.2141],\n",
            "        [0.2333, 0.1914, 0.1819, 0.1897, 0.2037],\n",
            "        [0.1964, 0.2095, 0.1890, 0.1940, 0.2111],\n",
            "        [0.1869, 0.2265, 0.1839, 0.1925, 0.2103],\n",
            "        [0.1890, 0.2157, 0.1797, 0.2092, 0.2064]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:23:48 PM | epoch   8 |   100/  103 batches | lr 20.00 | ms/batch 3301.02 | loss  7.66 | ppl  2113.69\n",
            "04/28 09:25:09 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:25:09 PM | end of epoch   8 | time: 419.62s | valid loss  6.88 | valid ppl   972.27\n",
            "04/28 09:25:09 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:25:09 PM Saving Normal!\n",
            "04/28 09:27:57 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2067, 0.2015, 0.1929, 0.2107, 0.1883],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1888, 0.2016],\n",
            "        [0.1669, 0.2338, 0.1792, 0.2206, 0.1994],\n",
            "        [0.2187, 0.2151, 0.1717, 0.1932, 0.2013],\n",
            "        [0.2146, 0.2176, 0.1707, 0.1973, 0.1997],\n",
            "        [0.1809, 0.2035, 0.2105, 0.1923, 0.2128],\n",
            "        [0.2232, 0.1985, 0.1958, 0.2051, 0.1774],\n",
            "        [0.2168, 0.1799, 0.1963, 0.1926, 0.2145],\n",
            "        [0.1705, 0.2058, 0.2127, 0.2146, 0.1964],\n",
            "        [0.1865, 0.2085, 0.1859, 0.2098, 0.2093],\n",
            "        [0.2060, 0.1987, 0.1872, 0.2073, 0.2008],\n",
            "        [0.2147, 0.1949, 0.1774, 0.2005, 0.2126],\n",
            "        [0.1745, 0.2039, 0.2000, 0.1982, 0.2234],\n",
            "        [0.2093, 0.1813, 0.1914, 0.1988, 0.2192],\n",
            "        [0.2010, 0.1913, 0.1931, 0.1880, 0.2265],\n",
            "        [0.2187, 0.1833, 0.1928, 0.1784, 0.2269],\n",
            "        [0.2019, 0.1838, 0.2013, 0.1807, 0.2323],\n",
            "        [0.1848, 0.1879, 0.2047, 0.1820, 0.2407],\n",
            "        [0.2283, 0.1797, 0.1872, 0.1820, 0.2229],\n",
            "        [0.1994, 0.1862, 0.1995, 0.1862, 0.2286],\n",
            "        [0.1899, 0.2073, 0.1854, 0.2006, 0.2169],\n",
            "        [0.2096, 0.2124, 0.1888, 0.1825, 0.2068],\n",
            "        [0.2051, 0.2010, 0.1893, 0.1878, 0.2169],\n",
            "        [0.1712, 0.2083, 0.1982, 0.1962, 0.2260],\n",
            "        [0.2080, 0.2074, 0.1778, 0.2000, 0.2068],\n",
            "        [0.1822, 0.2090, 0.1858, 0.2068, 0.2163],\n",
            "        [0.1997, 0.2123, 0.1855, 0.1879, 0.2145],\n",
            "        [0.1851, 0.2219, 0.1859, 0.1900, 0.2170],\n",
            "        [0.2240, 0.2029, 0.1798, 0.1944, 0.1989],\n",
            "        [0.2074, 0.2034, 0.1891, 0.1925, 0.2077],\n",
            "        [0.1768, 0.2253, 0.1900, 0.1937, 0.2141],\n",
            "        [0.2335, 0.1911, 0.1820, 0.1896, 0.2038],\n",
            "        [0.1963, 0.2094, 0.1892, 0.1940, 0.2110],\n",
            "        [0.1870, 0.2264, 0.1839, 0.1924, 0.2104],\n",
            "        [0.1890, 0.2157, 0.1797, 0.2092, 0.2064]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:27:57 PM | epoch   9 |    50/  103 batches | lr 20.00 | ms/batch 3368.94 | loss  7.79 | ppl  2407.59\n",
            "04/28 09:30:45 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2065, 0.2015, 0.1929, 0.2107, 0.1883],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1888, 0.2016],\n",
            "        [0.1669, 0.2338, 0.1792, 0.2206, 0.1995],\n",
            "        [0.2186, 0.2152, 0.1717, 0.1932, 0.2013],\n",
            "        [0.2147, 0.2176, 0.1707, 0.1973, 0.1997],\n",
            "        [0.1809, 0.2035, 0.2106, 0.1922, 0.2128],\n",
            "        [0.2231, 0.1985, 0.1957, 0.2054, 0.1773],\n",
            "        [0.2170, 0.1800, 0.1964, 0.1927, 0.2140],\n",
            "        [0.1705, 0.2056, 0.2134, 0.2143, 0.1963],\n",
            "        [0.1865, 0.2085, 0.1859, 0.2098, 0.2093],\n",
            "        [0.2058, 0.1990, 0.1872, 0.2073, 0.2007],\n",
            "        [0.2149, 0.1949, 0.1775, 0.2000, 0.2126],\n",
            "        [0.1745, 0.2040, 0.2001, 0.1980, 0.2234],\n",
            "        [0.2088, 0.1817, 0.1915, 0.1988, 0.2193],\n",
            "        [0.2010, 0.1913, 0.1931, 0.1881, 0.2265],\n",
            "        [0.2186, 0.1833, 0.1928, 0.1784, 0.2269],\n",
            "        [0.2019, 0.1838, 0.2013, 0.1807, 0.2323],\n",
            "        [0.1848, 0.1879, 0.2047, 0.1820, 0.2407],\n",
            "        [0.2283, 0.1796, 0.1872, 0.1820, 0.2229],\n",
            "        [0.1994, 0.1863, 0.1995, 0.1862, 0.2286],\n",
            "        [0.1899, 0.2073, 0.1855, 0.2004, 0.2169],\n",
            "        [0.2096, 0.2124, 0.1888, 0.1825, 0.2067],\n",
            "        [0.2054, 0.2009, 0.1892, 0.1877, 0.2168],\n",
            "        [0.1713, 0.2083, 0.1983, 0.1960, 0.2261],\n",
            "        [0.2079, 0.2074, 0.1778, 0.2001, 0.2068],\n",
            "        [0.1822, 0.2090, 0.1857, 0.2069, 0.2162],\n",
            "        [0.2006, 0.2135, 0.1864, 0.1889, 0.2106],\n",
            "        [0.1851, 0.2219, 0.1859, 0.1900, 0.2170],\n",
            "        [0.2241, 0.2026, 0.1799, 0.1945, 0.1990],\n",
            "        [0.2076, 0.2034, 0.1893, 0.1926, 0.2072],\n",
            "        [0.1769, 0.2253, 0.1900, 0.1937, 0.2141],\n",
            "        [0.2334, 0.1910, 0.1820, 0.1897, 0.2039],\n",
            "        [0.1963, 0.2094, 0.1893, 0.1939, 0.2110],\n",
            "        [0.1869, 0.2264, 0.1839, 0.1924, 0.2103],\n",
            "        [0.1889, 0.2158, 0.1798, 0.2090, 0.2065]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:30:45 PM | epoch   9 |   100/  103 batches | lr 20.00 | ms/batch 3357.31 | loss  7.49 | ppl  1784.36\n",
            "04/28 09:32:10 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:32:10 PM | end of epoch   9 | time: 421.14s | valid loss  6.84 | valid ppl   931.85\n",
            "04/28 09:32:10 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:32:10 PM Saving Normal!\n",
            "04/28 09:34:58 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('identity', 2), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2064, 0.2016, 0.1930, 0.2108, 0.1883],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1888, 0.2017],\n",
            "        [0.1669, 0.2337, 0.1792, 0.2205, 0.1997],\n",
            "        [0.2186, 0.2151, 0.1718, 0.1931, 0.2015],\n",
            "        [0.2147, 0.2175, 0.1707, 0.1972, 0.1998],\n",
            "        [0.1810, 0.2034, 0.2107, 0.1920, 0.2129],\n",
            "        [0.2233, 0.1986, 0.1959, 0.2046, 0.1776],\n",
            "        [0.2168, 0.1798, 0.1963, 0.1925, 0.2146],\n",
            "        [0.1706, 0.2057, 0.2129, 0.2144, 0.1964],\n",
            "        [0.1866, 0.2085, 0.1860, 0.2096, 0.2094],\n",
            "        [0.2060, 0.1985, 0.1873, 0.2071, 0.2011],\n",
            "        [0.2151, 0.1948, 0.1777, 0.1988, 0.2135],\n",
            "        [0.1747, 0.2041, 0.2001, 0.1974, 0.2237],\n",
            "        [0.2089, 0.1795, 0.1921, 0.1994, 0.2200],\n",
            "        [0.2011, 0.1913, 0.1931, 0.1880, 0.2265],\n",
            "        [0.2186, 0.1833, 0.1928, 0.1784, 0.2269],\n",
            "        [0.2020, 0.1838, 0.2012, 0.1807, 0.2323],\n",
            "        [0.1848, 0.1877, 0.2048, 0.1819, 0.2407],\n",
            "        [0.2280, 0.1797, 0.1873, 0.1821, 0.2230],\n",
            "        [0.1995, 0.1862, 0.1995, 0.1862, 0.2286],\n",
            "        [0.1899, 0.2074, 0.1856, 0.2002, 0.2170],\n",
            "        [0.2096, 0.2124, 0.1888, 0.1825, 0.2067],\n",
            "        [0.2058, 0.2008, 0.1892, 0.1876, 0.2167],\n",
            "        [0.1713, 0.2083, 0.1984, 0.1959, 0.2260],\n",
            "        [0.2077, 0.2074, 0.1779, 0.2000, 0.2071],\n",
            "        [0.1822, 0.2090, 0.1858, 0.2068, 0.2162],\n",
            "        [0.2009, 0.2137, 0.1868, 0.1891, 0.2095],\n",
            "        [0.1853, 0.2218, 0.1859, 0.1899, 0.2171],\n",
            "        [0.2242, 0.2023, 0.1799, 0.1945, 0.1992],\n",
            "        [0.2063, 0.2020, 0.1880, 0.1912, 0.2125],\n",
            "        [0.1770, 0.2252, 0.1900, 0.1935, 0.2143],\n",
            "        [0.2329, 0.1917, 0.1820, 0.1897, 0.2039],\n",
            "        [0.1963, 0.2094, 0.1895, 0.1936, 0.2112],\n",
            "        [0.1872, 0.2263, 0.1839, 0.1923, 0.2103],\n",
            "        [0.1893, 0.2160, 0.1809, 0.2060, 0.2078]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:34:58 PM | epoch  10 |    50/  103 batches | lr 20.00 | ms/batch 3357.07 | loss  7.64 | ppl  2070.71\n",
            "04/28 09:37:40 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('identity', 2), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2063, 0.2016, 0.1930, 0.2108, 0.1883],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1888, 0.2016],\n",
            "        [0.1669, 0.2337, 0.1792, 0.2205, 0.1996],\n",
            "        [0.2184, 0.2151, 0.1718, 0.1931, 0.2015],\n",
            "        [0.2147, 0.2175, 0.1707, 0.1972, 0.1998],\n",
            "        [0.1810, 0.2034, 0.2106, 0.1920, 0.2130],\n",
            "        [0.2233, 0.1986, 0.1960, 0.2045, 0.1775],\n",
            "        [0.2167, 0.1797, 0.1962, 0.1924, 0.2149],\n",
            "        [0.1704, 0.2057, 0.2132, 0.2146, 0.1962],\n",
            "        [0.1866, 0.2085, 0.1860, 0.2096, 0.2094],\n",
            "        [0.2060, 0.1984, 0.1874, 0.2070, 0.2012],\n",
            "        [0.2151, 0.1949, 0.1776, 0.1993, 0.2131],\n",
            "        [0.1745, 0.2040, 0.2000, 0.1979, 0.2235],\n",
            "        [0.2087, 0.1802, 0.1919, 0.1993, 0.2199],\n",
            "        [0.2012, 0.1913, 0.1931, 0.1880, 0.2264],\n",
            "        [0.2186, 0.1833, 0.1928, 0.1784, 0.2270],\n",
            "        [0.2020, 0.1838, 0.2012, 0.1807, 0.2323],\n",
            "        [0.1847, 0.1878, 0.2048, 0.1819, 0.2407],\n",
            "        [0.2281, 0.1797, 0.1872, 0.1820, 0.2229],\n",
            "        [0.1995, 0.1863, 0.1995, 0.1862, 0.2286],\n",
            "        [0.1899, 0.2073, 0.1856, 0.2003, 0.2169],\n",
            "        [0.2096, 0.2124, 0.1889, 0.1825, 0.2066],\n",
            "        [0.2059, 0.2007, 0.1892, 0.1875, 0.2166],\n",
            "        [0.1713, 0.2083, 0.1984, 0.1961, 0.2259],\n",
            "        [0.2078, 0.2075, 0.1780, 0.2001, 0.2066],\n",
            "        [0.1820, 0.2088, 0.1855, 0.2078, 0.2159],\n",
            "        [0.2009, 0.2134, 0.1867, 0.1890, 0.2099],\n",
            "        [0.1853, 0.2217, 0.1859, 0.1899, 0.2172],\n",
            "        [0.2242, 0.2021, 0.1799, 0.1945, 0.1992],\n",
            "        [0.2071, 0.2028, 0.1887, 0.1919, 0.2095],\n",
            "        [0.1770, 0.2253, 0.1900, 0.1936, 0.2142],\n",
            "        [0.2329, 0.1916, 0.1819, 0.1896, 0.2039],\n",
            "        [0.1963, 0.2093, 0.1897, 0.1934, 0.2112],\n",
            "        [0.1873, 0.2262, 0.1839, 0.1923, 0.2103],\n",
            "        [0.1895, 0.2160, 0.1811, 0.2053, 0.2081]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:37:40 PM | epoch  10 |   100/  103 batches | lr 20.00 | ms/batch 3254.60 | loss  7.36 | ppl  1574.94\n",
            "04/28 09:39:05 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:39:05 PM | end of epoch  10 | time: 415.16s | valid loss  6.82 | valid ppl   916.70\n",
            "04/28 09:39:05 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:39:05 PM Saving Normal!\n",
            "04/28 09:41:58 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2061, 0.2016, 0.1930, 0.2108, 0.1884],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1888, 0.2016],\n",
            "        [0.1669, 0.2337, 0.1792, 0.2205, 0.1996],\n",
            "        [0.2184, 0.2151, 0.1718, 0.1931, 0.2015],\n",
            "        [0.2147, 0.2175, 0.1708, 0.1972, 0.1998],\n",
            "        [0.1809, 0.2035, 0.2106, 0.1921, 0.2129],\n",
            "        [0.2227, 0.1983, 0.1955, 0.2065, 0.1770],\n",
            "        [0.2168, 0.1798, 0.1962, 0.1925, 0.2147],\n",
            "        [0.1706, 0.2061, 0.2117, 0.2156, 0.1960],\n",
            "        [0.1866, 0.2085, 0.1860, 0.2096, 0.2093],\n",
            "        [0.2066, 0.1976, 0.1875, 0.2069, 0.2014],\n",
            "        [0.2152, 0.1948, 0.1777, 0.1994, 0.2129],\n",
            "        [0.1744, 0.2040, 0.2001, 0.1979, 0.2236],\n",
            "        [0.2062, 0.1848, 0.1913, 0.1987, 0.2191],\n",
            "        [0.2014, 0.1912, 0.1930, 0.1880, 0.2264],\n",
            "        [0.2186, 0.1833, 0.1927, 0.1784, 0.2270],\n",
            "        [0.2020, 0.1838, 0.2012, 0.1807, 0.2323],\n",
            "        [0.1847, 0.1878, 0.2048, 0.1820, 0.2407],\n",
            "        [0.2279, 0.1797, 0.1873, 0.1821, 0.2230],\n",
            "        [0.1995, 0.1863, 0.1994, 0.1863, 0.2286],\n",
            "        [0.1900, 0.2074, 0.1856, 0.2002, 0.2169],\n",
            "        [0.2096, 0.2123, 0.1889, 0.1825, 0.2068],\n",
            "        [0.2059, 0.2007, 0.1892, 0.1875, 0.2166],\n",
            "        [0.1713, 0.2083, 0.1984, 0.1962, 0.2258],\n",
            "        [0.2077, 0.2074, 0.1780, 0.2000, 0.2069],\n",
            "        [0.1822, 0.2090, 0.1858, 0.2067, 0.2163],\n",
            "        [0.1976, 0.2095, 0.1837, 0.1858, 0.2235],\n",
            "        [0.1854, 0.2218, 0.1859, 0.1899, 0.2171],\n",
            "        [0.2245, 0.2016, 0.1800, 0.1946, 0.1993],\n",
            "        [0.2081, 0.2036, 0.1895, 0.1928, 0.2060],\n",
            "        [0.1768, 0.2253, 0.1900, 0.1937, 0.2141],\n",
            "        [0.2323, 0.1917, 0.1820, 0.1897, 0.2042],\n",
            "        [0.1963, 0.2093, 0.1898, 0.1935, 0.2111],\n",
            "        [0.1872, 0.2262, 0.1839, 0.1923, 0.2103],\n",
            "        [0.1895, 0.2161, 0.1809, 0.2051, 0.2084]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:41:58 PM | epoch  11 |    50/  103 batches | lr 20.00 | ms/batch 3448.72 | loss  7.51 | ppl  1821.82\n",
            "04/28 09:44:43 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2060, 0.2017, 0.1930, 0.2109, 0.1885],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1889, 0.2016],\n",
            "        [0.1669, 0.2337, 0.1792, 0.2205, 0.1996],\n",
            "        [0.2183, 0.2151, 0.1718, 0.1932, 0.2016],\n",
            "        [0.2147, 0.2175, 0.1707, 0.1972, 0.1999],\n",
            "        [0.1809, 0.2034, 0.2107, 0.1921, 0.2129],\n",
            "        [0.2226, 0.1983, 0.1955, 0.2067, 0.1769],\n",
            "        [0.2168, 0.1798, 0.1963, 0.1926, 0.2145],\n",
            "        [0.1705, 0.2059, 0.2123, 0.2155, 0.1959],\n",
            "        [0.1867, 0.2085, 0.1860, 0.2095, 0.2094],\n",
            "        [0.2065, 0.1975, 0.1875, 0.2069, 0.2016],\n",
            "        [0.2152, 0.1948, 0.1777, 0.1995, 0.2128],\n",
            "        [0.1745, 0.2040, 0.2001, 0.1976, 0.2238],\n",
            "        [0.2066, 0.1848, 0.1911, 0.1986, 0.2189],\n",
            "        [0.2014, 0.1912, 0.1930, 0.1880, 0.2264],\n",
            "        [0.2186, 0.1833, 0.1928, 0.1784, 0.2270],\n",
            "        [0.2020, 0.1838, 0.2012, 0.1807, 0.2323],\n",
            "        [0.1847, 0.1878, 0.2048, 0.1820, 0.2407],\n",
            "        [0.2278, 0.1798, 0.1873, 0.1821, 0.2230],\n",
            "        [0.1995, 0.1863, 0.1994, 0.1863, 0.2286],\n",
            "        [0.1900, 0.2074, 0.1856, 0.1999, 0.2170],\n",
            "        [0.2095, 0.2123, 0.1888, 0.1824, 0.2071],\n",
            "        [0.2056, 0.2008, 0.1892, 0.1876, 0.2168],\n",
            "        [0.1713, 0.2083, 0.1984, 0.1961, 0.2259],\n",
            "        [0.2080, 0.2074, 0.1779, 0.2000, 0.2067],\n",
            "        [0.1823, 0.2090, 0.1858, 0.2065, 0.2164],\n",
            "        [0.1990, 0.2112, 0.1850, 0.1872, 0.2176],\n",
            "        [0.1854, 0.2217, 0.1858, 0.1899, 0.2172],\n",
            "        [0.2246, 0.2014, 0.1800, 0.1946, 0.1994],\n",
            "        [0.2072, 0.2028, 0.1888, 0.1920, 0.2092],\n",
            "        [0.1768, 0.2253, 0.1900, 0.1937, 0.2142],\n",
            "        [0.2325, 0.1912, 0.1821, 0.1897, 0.2044],\n",
            "        [0.1963, 0.2093, 0.1898, 0.1935, 0.2111],\n",
            "        [0.1871, 0.2263, 0.1839, 0.1924, 0.2102],\n",
            "        [0.1895, 0.2160, 0.1808, 0.2056, 0.2081]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:44:43 PM | epoch  11 |   100/  103 batches | lr 20.00 | ms/batch 3298.42 | loss  7.27 | ppl  1431.52\n",
            "04/28 09:46:01 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:46:01 PM | end of epoch  11 | time: 416.15s | valid loss  6.79 | valid ppl   887.22\n",
            "04/28 09:46:01 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:46:01 PM Saving Normal!\n",
            "04/28 09:48:42 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2058, 0.2017, 0.1931, 0.2109, 0.1885],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1888, 0.2016],\n",
            "        [0.1670, 0.2337, 0.1792, 0.2205, 0.1996],\n",
            "        [0.2181, 0.2152, 0.1718, 0.1932, 0.2016],\n",
            "        [0.2147, 0.2175, 0.1707, 0.1972, 0.1999],\n",
            "        [0.1808, 0.2035, 0.2107, 0.1922, 0.2128],\n",
            "        [0.2225, 0.1981, 0.1954, 0.2073, 0.1767],\n",
            "        [0.2170, 0.1800, 0.1965, 0.1927, 0.2138],\n",
            "        [0.1705, 0.2058, 0.2128, 0.2151, 0.1958],\n",
            "        [0.1866, 0.2085, 0.1860, 0.2096, 0.2094],\n",
            "        [0.2064, 0.1975, 0.1875, 0.2068, 0.2018],\n",
            "        [0.2150, 0.1947, 0.1776, 0.1988, 0.2139],\n",
            "        [0.1746, 0.2040, 0.2001, 0.1976, 0.2238],\n",
            "        [0.2075, 0.1845, 0.1909, 0.1984, 0.2187],\n",
            "        [0.2014, 0.1912, 0.1930, 0.1880, 0.2264],\n",
            "        [0.2186, 0.1833, 0.1928, 0.1784, 0.2269],\n",
            "        [0.2020, 0.1838, 0.2013, 0.1807, 0.2323],\n",
            "        [0.1847, 0.1877, 0.2049, 0.1819, 0.2407],\n",
            "        [0.2279, 0.1798, 0.1873, 0.1821, 0.2229],\n",
            "        [0.1995, 0.1863, 0.1994, 0.1863, 0.2285],\n",
            "        [0.1900, 0.2074, 0.1856, 0.1999, 0.2170],\n",
            "        [0.2094, 0.2124, 0.1887, 0.1824, 0.2071],\n",
            "        [0.2046, 0.2012, 0.1894, 0.1879, 0.2169],\n",
            "        [0.1714, 0.2083, 0.1984, 0.1959, 0.2260],\n",
            "        [0.2085, 0.2074, 0.1780, 0.2000, 0.2061],\n",
            "        [0.1819, 0.2088, 0.1855, 0.2078, 0.2159],\n",
            "        [0.2008, 0.2138, 0.1863, 0.1889, 0.2102],\n",
            "        [0.1854, 0.2216, 0.1859, 0.1898, 0.2172],\n",
            "        [0.2243, 0.2015, 0.1800, 0.1946, 0.1996],\n",
            "        [0.2075, 0.2031, 0.1890, 0.1923, 0.2080],\n",
            "        [0.1770, 0.2253, 0.1900, 0.1936, 0.2141],\n",
            "        [0.2325, 0.1914, 0.1821, 0.1897, 0.2043],\n",
            "        [0.1963, 0.2094, 0.1894, 0.1939, 0.2110],\n",
            "        [0.1870, 0.2265, 0.1839, 0.1925, 0.2101],\n",
            "        [0.1895, 0.2160, 0.1809, 0.2058, 0.2078]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:48:42 PM | epoch  12 |    50/  103 batches | lr 20.00 | ms/batch 3219.44 | loss  7.33 | ppl  1523.98\n",
            "04/28 09:51:22 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2057, 0.2017, 0.1931, 0.2109, 0.1885],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1889, 0.2016],\n",
            "        [0.1670, 0.2337, 0.1791, 0.2205, 0.1998],\n",
            "        [0.2182, 0.2151, 0.1718, 0.1932, 0.2017],\n",
            "        [0.2147, 0.2175, 0.1707, 0.1972, 0.1999],\n",
            "        [0.1809, 0.2035, 0.2107, 0.1922, 0.2128],\n",
            "        [0.2228, 0.1983, 0.1956, 0.2063, 0.1769],\n",
            "        [0.2170, 0.1800, 0.1965, 0.1928, 0.2137],\n",
            "        [0.1705, 0.2059, 0.2122, 0.2154, 0.1960],\n",
            "        [0.1867, 0.2085, 0.1860, 0.2094, 0.2095],\n",
            "        [0.2064, 0.1972, 0.1876, 0.2068, 0.2020],\n",
            "        [0.2150, 0.1945, 0.1776, 0.1983, 0.2145],\n",
            "        [0.1745, 0.2040, 0.2001, 0.1976, 0.2238],\n",
            "        [0.2091, 0.1817, 0.1913, 0.1987, 0.2191],\n",
            "        [0.2015, 0.1912, 0.1931, 0.1880, 0.2263],\n",
            "        [0.2185, 0.1833, 0.1928, 0.1784, 0.2270],\n",
            "        [0.2019, 0.1838, 0.2013, 0.1807, 0.2323],\n",
            "        [0.1847, 0.1878, 0.2049, 0.1819, 0.2407],\n",
            "        [0.2281, 0.1798, 0.1872, 0.1821, 0.2228],\n",
            "        [0.1995, 0.1864, 0.1993, 0.1863, 0.2285],\n",
            "        [0.1899, 0.2074, 0.1855, 0.2002, 0.2170],\n",
            "        [0.2094, 0.2123, 0.1887, 0.1824, 0.2072],\n",
            "        [0.2046, 0.2013, 0.1893, 0.1879, 0.2169],\n",
            "        [0.1713, 0.2083, 0.1983, 0.1960, 0.2260],\n",
            "        [0.2085, 0.2075, 0.1781, 0.2001, 0.2060],\n",
            "        [0.1818, 0.2085, 0.1852, 0.2091, 0.2154],\n",
            "        [0.1984, 0.2107, 0.1842, 0.1866, 0.2201],\n",
            "        [0.1856, 0.2216, 0.1859, 0.1898, 0.2172],\n",
            "        [0.2244, 0.2014, 0.1801, 0.1946, 0.1995],\n",
            "        [0.2075, 0.2032, 0.1891, 0.1924, 0.2077],\n",
            "        [0.1769, 0.2254, 0.1900, 0.1939, 0.2139],\n",
            "        [0.2323, 0.1914, 0.1822, 0.1897, 0.2044],\n",
            "        [0.1964, 0.2094, 0.1893, 0.1939, 0.2110],\n",
            "        [0.1867, 0.2264, 0.1841, 0.1925, 0.2104],\n",
            "        [0.1898, 0.2160, 0.1811, 0.2049, 0.2081]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:51:22 PM | epoch  12 |   100/  103 batches | lr 20.00 | ms/batch 3189.45 | loss  7.08 | ppl  1186.28\n",
            "04/28 09:52:43 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:52:43 PM | end of epoch  12 | time: 402.06s | valid loss  6.73 | valid ppl   836.74\n",
            "04/28 09:52:43 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:52:44 PM Saving Normal!\n",
            "04/28 09:55:22 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2055, 0.2018, 0.1932, 0.2110, 0.1885],\n",
            "        [0.1812, 0.2328, 0.1858, 0.1907, 0.2096],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1888, 0.2016],\n",
            "        [0.1669, 0.2336, 0.1791, 0.2205, 0.1998],\n",
            "        [0.2185, 0.2151, 0.1718, 0.1931, 0.2016],\n",
            "        [0.2146, 0.2175, 0.1707, 0.1972, 0.1999],\n",
            "        [0.1809, 0.2035, 0.2107, 0.1922, 0.2128],\n",
            "        [0.2228, 0.1983, 0.1956, 0.2065, 0.1769],\n",
            "        [0.2172, 0.1803, 0.1967, 0.1930, 0.2128],\n",
            "        [0.1705, 0.2059, 0.2125, 0.2153, 0.1959],\n",
            "        [0.1866, 0.2085, 0.1860, 0.2095, 0.2094],\n",
            "        [0.2063, 0.1977, 0.1875, 0.2069, 0.2017],\n",
            "        [0.2149, 0.1945, 0.1776, 0.1983, 0.2146],\n",
            "        [0.1744, 0.2040, 0.2000, 0.1979, 0.2237],\n",
            "        [0.2097, 0.1813, 0.1913, 0.1987, 0.2190],\n",
            "        [0.2015, 0.1913, 0.1931, 0.1879, 0.2263],\n",
            "        [0.2186, 0.1833, 0.1929, 0.1784, 0.2269],\n",
            "        [0.2019, 0.1838, 0.2013, 0.1807, 0.2322],\n",
            "        [0.1846, 0.1878, 0.2050, 0.1819, 0.2406],\n",
            "        [0.2282, 0.1798, 0.1872, 0.1821, 0.2227],\n",
            "        [0.1995, 0.1864, 0.1993, 0.1863, 0.2285],\n",
            "        [0.1900, 0.2074, 0.1856, 0.1999, 0.2170],\n",
            "        [0.2093, 0.2122, 0.1887, 0.1823, 0.2075],\n",
            "        [0.2046, 0.2012, 0.1894, 0.1879, 0.2170],\n",
            "        [0.1715, 0.2083, 0.1983, 0.1957, 0.2261],\n",
            "        [0.2083, 0.2074, 0.1780, 0.1999, 0.2063],\n",
            "        [0.1817, 0.2085, 0.1851, 0.2093, 0.2154],\n",
            "        [0.1985, 0.2111, 0.1843, 0.1868, 0.2193],\n",
            "        [0.1857, 0.2216, 0.1858, 0.1898, 0.2171],\n",
            "        [0.2247, 0.2008, 0.1803, 0.1947, 0.1995],\n",
            "        [0.2082, 0.2038, 0.1897, 0.1929, 0.2054],\n",
            "        [0.1769, 0.2253, 0.1900, 0.1937, 0.2141],\n",
            "        [0.2317, 0.1916, 0.1822, 0.1898, 0.2047],\n",
            "        [0.1964, 0.2094, 0.1893, 0.1938, 0.2111],\n",
            "        [0.1865, 0.2263, 0.1843, 0.1924, 0.2105],\n",
            "        [0.1896, 0.2159, 0.1808, 0.2063, 0.2074]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:55:22 PM | epoch  13 |    50/  103 batches | lr 20.00 | ms/batch 3165.06 | loss  7.18 | ppl  1306.83\n",
            "04/28 09:58:00 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2054, 0.2018, 0.1932, 0.2110, 0.1885],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2156, 0.2228, 0.1712, 0.1889, 0.2016],\n",
            "        [0.1669, 0.2336, 0.1791, 0.2204, 0.1999],\n",
            "        [0.2186, 0.2150, 0.1717, 0.1930, 0.2016],\n",
            "        [0.2146, 0.2175, 0.1707, 0.1972, 0.2000],\n",
            "        [0.1808, 0.2035, 0.2106, 0.1923, 0.2128],\n",
            "        [0.2226, 0.1982, 0.1955, 0.2067, 0.1769],\n",
            "        [0.2171, 0.1802, 0.1967, 0.1929, 0.2131],\n",
            "        [0.1707, 0.2060, 0.2117, 0.2155, 0.1960],\n",
            "        [0.1866, 0.2084, 0.1859, 0.2098, 0.2093],\n",
            "        [0.2061, 0.1979, 0.1874, 0.2070, 0.2016],\n",
            "        [0.2150, 0.1945, 0.1776, 0.1984, 0.2144],\n",
            "        [0.1745, 0.2040, 0.2001, 0.1978, 0.2237],\n",
            "        [0.2086, 0.1835, 0.1910, 0.1984, 0.2186],\n",
            "        [0.2016, 0.1913, 0.1930, 0.1879, 0.2262],\n",
            "        [0.2186, 0.1832, 0.1929, 0.1783, 0.2269],\n",
            "        [0.2018, 0.1839, 0.2014, 0.1807, 0.2322],\n",
            "        [0.1847, 0.1878, 0.2050, 0.1819, 0.2407],\n",
            "        [0.2282, 0.1798, 0.1873, 0.1821, 0.2227],\n",
            "        [0.1995, 0.1864, 0.1993, 0.1863, 0.2285],\n",
            "        [0.1898, 0.2073, 0.1853, 0.2009, 0.2166],\n",
            "        [0.2093, 0.2124, 0.1887, 0.1824, 0.2072],\n",
            "        [0.2047, 0.2011, 0.1893, 0.1878, 0.2170],\n",
            "        [0.1715, 0.2083, 0.1984, 0.1957, 0.2260],\n",
            "        [0.2079, 0.2075, 0.1781, 0.2001, 0.2064],\n",
            "        [0.1815, 0.2084, 0.1851, 0.2098, 0.2152],\n",
            "        [0.1994, 0.2119, 0.1850, 0.1875, 0.2162],\n",
            "        [0.1857, 0.2217, 0.1857, 0.1899, 0.2171],\n",
            "        [0.2249, 0.2005, 0.1802, 0.1948, 0.1996],\n",
            "        [0.2082, 0.2040, 0.1897, 0.1931, 0.2050],\n",
            "        [0.1770, 0.2253, 0.1901, 0.1936, 0.2141],\n",
            "        [0.2316, 0.1921, 0.1821, 0.1898, 0.2044],\n",
            "        [0.1962, 0.2093, 0.1898, 0.1938, 0.2109],\n",
            "        [0.1865, 0.2260, 0.1845, 0.1923, 0.2108],\n",
            "        [0.1896, 0.2160, 0.1806, 0.2059, 0.2079]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 09:58:00 PM | epoch  13 |   100/  103 batches | lr 20.00 | ms/batch 3172.36 | loss  6.82 | ppl   916.34\n",
            "04/28 09:59:19 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:59:19 PM | end of epoch  13 | time: 395.53s | valid loss  6.73 | valid ppl   834.20\n",
            "04/28 09:59:19 PM -----------------------------------------------------------------------------------------\n",
            "04/28 09:59:19 PM Saving Normal!\n",
            "04/28 10:02:03 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2053, 0.2019, 0.1933, 0.2111, 0.1884],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2155, 0.2229, 0.1712, 0.1889, 0.2015],\n",
            "        [0.1670, 0.2338, 0.1792, 0.2206, 0.1995],\n",
            "        [0.2186, 0.2152, 0.1717, 0.1931, 0.2014],\n",
            "        [0.2146, 0.2175, 0.1708, 0.1973, 0.1999],\n",
            "        [0.1808, 0.2036, 0.2105, 0.1924, 0.2127],\n",
            "        [0.2221, 0.1979, 0.1950, 0.2086, 0.1764],\n",
            "        [0.2170, 0.1802, 0.1966, 0.1929, 0.2133],\n",
            "        [0.1708, 0.2062, 0.2115, 0.2156, 0.1959],\n",
            "        [0.1865, 0.2085, 0.1858, 0.2100, 0.2092],\n",
            "        [0.2060, 0.1982, 0.1873, 0.2071, 0.2015],\n",
            "        [0.2147, 0.1946, 0.1774, 0.1992, 0.2141],\n",
            "        [0.1745, 0.2039, 0.1999, 0.1982, 0.2235],\n",
            "        [0.2090, 0.1831, 0.1909, 0.1984, 0.2186],\n",
            "        [0.2015, 0.1915, 0.1930, 0.1880, 0.2261],\n",
            "        [0.2187, 0.1833, 0.1929, 0.1784, 0.2267],\n",
            "        [0.2017, 0.1840, 0.2014, 0.1808, 0.2321],\n",
            "        [0.1846, 0.1878, 0.2050, 0.1819, 0.2406],\n",
            "        [0.2281, 0.1798, 0.1873, 0.1821, 0.2228],\n",
            "        [0.1995, 0.1863, 0.1993, 0.1863, 0.2286],\n",
            "        [0.1897, 0.2072, 0.1852, 0.2015, 0.2164],\n",
            "        [0.2094, 0.2125, 0.1887, 0.1825, 0.2069],\n",
            "        [0.2040, 0.2013, 0.1894, 0.1880, 0.2174],\n",
            "        [0.1716, 0.2083, 0.1985, 0.1955, 0.2261],\n",
            "        [0.2078, 0.2073, 0.1780, 0.1999, 0.2071],\n",
            "        [0.1810, 0.2080, 0.1844, 0.2121, 0.2145],\n",
            "        [0.1988, 0.2111, 0.1844, 0.1869, 0.2187],\n",
            "        [0.1856, 0.2220, 0.1853, 0.1903, 0.2168],\n",
            "        [0.2248, 0.2009, 0.1800, 0.1947, 0.1995],\n",
            "        [0.2069, 0.2029, 0.1885, 0.1920, 0.2097],\n",
            "        [0.1771, 0.2251, 0.1901, 0.1933, 0.2144],\n",
            "        [0.2319, 0.1927, 0.1817, 0.1897, 0.2039],\n",
            "        [0.1961, 0.2095, 0.1892, 0.1949, 0.2104],\n",
            "        [0.1866, 0.2265, 0.1844, 0.1925, 0.2099],\n",
            "        [0.1899, 0.2162, 0.1814, 0.2025, 0.2101]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 10:02:03 PM | epoch  14 |    50/  103 batches | lr 20.00 | ms/batch 3273.41 | loss  6.90 | ppl   996.95\n",
            "04/28 10:04:47 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2052, 0.2020, 0.1934, 0.2112, 0.1883],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2155, 0.2229, 0.1712, 0.1890, 0.2014],\n",
            "        [0.1669, 0.2337, 0.1792, 0.2205, 0.1997],\n",
            "        [0.2185, 0.2152, 0.1717, 0.1932, 0.2014],\n",
            "        [0.2147, 0.2175, 0.1707, 0.1972, 0.1998],\n",
            "        [0.1808, 0.2038, 0.2104, 0.1924, 0.2127],\n",
            "        [0.2218, 0.1978, 0.1948, 0.2094, 0.1761],\n",
            "        [0.2172, 0.1804, 0.1967, 0.1931, 0.2126],\n",
            "        [0.1708, 0.2062, 0.2113, 0.2155, 0.1961],\n",
            "        [0.1865, 0.2085, 0.1859, 0.2097, 0.2094],\n",
            "        [0.2061, 0.1978, 0.1874, 0.2069, 0.2018],\n",
            "        [0.2150, 0.1945, 0.1777, 0.1978, 0.2150],\n",
            "        [0.1743, 0.2038, 0.1998, 0.1988, 0.2232],\n",
            "        [0.2074, 0.1920, 0.1887, 0.1962, 0.2157],\n",
            "        [0.2013, 0.1915, 0.1929, 0.1880, 0.2263],\n",
            "        [0.2188, 0.1833, 0.1928, 0.1784, 0.2268],\n",
            "        [0.2019, 0.1840, 0.2013, 0.1808, 0.2321],\n",
            "        [0.1846, 0.1877, 0.2051, 0.1819, 0.2407],\n",
            "        [0.2281, 0.1798, 0.1873, 0.1821, 0.2227],\n",
            "        [0.1995, 0.1863, 0.1993, 0.1863, 0.2286],\n",
            "        [0.1895, 0.2071, 0.1850, 0.2022, 0.2162],\n",
            "        [0.2095, 0.2128, 0.1888, 0.1826, 0.2064],\n",
            "        [0.2042, 0.2011, 0.1895, 0.1878, 0.2174],\n",
            "        [0.1713, 0.2083, 0.1984, 0.1959, 0.2260],\n",
            "        [0.2074, 0.2071, 0.1779, 0.1996, 0.2080],\n",
            "        [0.1808, 0.2077, 0.1842, 0.2133, 0.2140],\n",
            "        [0.1999, 0.2130, 0.1850, 0.1880, 0.2141],\n",
            "        [0.1857, 0.2220, 0.1855, 0.1903, 0.2165],\n",
            "        [0.2250, 0.2007, 0.1803, 0.1948, 0.1993],\n",
            "        [0.2093, 0.2050, 0.1907, 0.1941, 0.2009],\n",
            "        [0.1772, 0.2249, 0.1905, 0.1931, 0.2144],\n",
            "        [0.2318, 0.1917, 0.1820, 0.1898, 0.2047],\n",
            "        [0.1961, 0.2095, 0.1890, 0.1952, 0.2103],\n",
            "        [0.1863, 0.2263, 0.1843, 0.1924, 0.2107],\n",
            "        [0.1902, 0.2163, 0.1818, 0.2003, 0.2114]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 10:04:47 PM | epoch  14 |   100/  103 batches | lr 20.00 | ms/batch 3289.38 | loss  6.65 | ppl   774.24\n",
            "04/28 10:06:00 PM -----------------------------------------------------------------------------------------\n",
            "04/28 10:06:00 PM | end of epoch  14 | time: 401.13s | valid loss  6.68 | valid ppl   793.09\n",
            "04/28 10:06:00 PM -----------------------------------------------------------------------------------------\n",
            "04/28 10:06:00 PM Saving Normal!\n",
            "04/28 10:08:44 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2050, 0.2020, 0.1934, 0.2112, 0.1883],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2155, 0.2229, 0.1712, 0.1889, 0.2015],\n",
            "        [0.1668, 0.2337, 0.1791, 0.2205, 0.1999],\n",
            "        [0.2185, 0.2152, 0.1717, 0.1931, 0.2015],\n",
            "        [0.2147, 0.2175, 0.1707, 0.1972, 0.1998],\n",
            "        [0.1808, 0.2036, 0.2105, 0.1922, 0.2128],\n",
            "        [0.2223, 0.1982, 0.1952, 0.2078, 0.1766],\n",
            "        [0.2171, 0.1802, 0.1966, 0.1929, 0.2132],\n",
            "        [0.1707, 0.2058, 0.2127, 0.2146, 0.1960],\n",
            "        [0.1866, 0.2085, 0.1860, 0.2095, 0.2095],\n",
            "        [0.2063, 0.1976, 0.1874, 0.2068, 0.2019],\n",
            "        [0.2152, 0.1945, 0.1777, 0.1978, 0.2148],\n",
            "        [0.1745, 0.2038, 0.1999, 0.1985, 0.2233],\n",
            "        [0.2079, 0.1852, 0.1906, 0.1981, 0.2183],\n",
            "        [0.2011, 0.1915, 0.1930, 0.1881, 0.2263],\n",
            "        [0.2188, 0.1832, 0.1928, 0.1783, 0.2268],\n",
            "        [0.2020, 0.1840, 0.2012, 0.1808, 0.2320],\n",
            "        [0.1846, 0.1877, 0.2050, 0.1820, 0.2408],\n",
            "        [0.2280, 0.1799, 0.1872, 0.1821, 0.2228],\n",
            "        [0.1995, 0.1863, 0.1992, 0.1863, 0.2287],\n",
            "        [0.1894, 0.2070, 0.1849, 0.2025, 0.2161],\n",
            "        [0.2095, 0.2128, 0.1888, 0.1826, 0.2063],\n",
            "        [0.2041, 0.2010, 0.1894, 0.1878, 0.2176],\n",
            "        [0.1713, 0.2083, 0.1982, 0.1965, 0.2257],\n",
            "        [0.2078, 0.2075, 0.1782, 0.2001, 0.2064],\n",
            "        [0.1809, 0.2079, 0.1844, 0.2125, 0.2143],\n",
            "        [0.1998, 0.2126, 0.1850, 0.1878, 0.2148],\n",
            "        [0.1858, 0.2219, 0.1857, 0.1903, 0.2164],\n",
            "        [0.2254, 0.1999, 0.1806, 0.1950, 0.1991],\n",
            "        [0.2097, 0.2052, 0.1910, 0.1943, 0.1998],\n",
            "        [0.1773, 0.2248, 0.1906, 0.1930, 0.2144],\n",
            "        [0.2321, 0.1906, 0.1824, 0.1898, 0.2051],\n",
            "        [0.1961, 0.2095, 0.1888, 0.1955, 0.2101],\n",
            "        [0.1859, 0.2254, 0.1846, 0.1920, 0.2122],\n",
            "        [0.1901, 0.2164, 0.1822, 0.1991, 0.2122]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 10:08:44 PM | epoch  15 |    50/  103 batches | lr 20.00 | ms/batch 3271.02 | loss  6.71 | ppl   818.22\n",
            "04/28 10:11:23 PM Genotype(recurrent=[('sigmoid', 0), ('tanh', 0), ('tanh', 0), ('sigmoid', 3), ('identity', 3), ('identity', 3), ('identity', 3), ('tanh', 6)], concat=range(1, 9))\n",
            "tensor([[0.2048, 0.2021, 0.1934, 0.2113, 0.1884],\n",
            "        [0.1812, 0.2328, 0.1857, 0.1907, 0.2096],\n",
            "        [0.2155, 0.2229, 0.1712, 0.1889, 0.2015],\n",
            "        [0.1668, 0.2338, 0.1792, 0.2206, 0.1996],\n",
            "        [0.2185, 0.2154, 0.1716, 0.1932, 0.2013],\n",
            "        [0.2147, 0.2176, 0.1707, 0.1972, 0.1997],\n",
            "        [0.1809, 0.2037, 0.2104, 0.1921, 0.2129],\n",
            "        [0.2220, 0.1980, 0.1949, 0.2084, 0.1767],\n",
            "        [0.2169, 0.1801, 0.1964, 0.1927, 0.2139],\n",
            "        [0.1711, 0.2059, 0.2124, 0.2142, 0.1964],\n",
            "        [0.1866, 0.2085, 0.1860, 0.2094, 0.2095],\n",
            "        [0.2060, 0.1975, 0.1876, 0.2067, 0.2022],\n",
            "        [0.2154, 0.1944, 0.1779, 0.1968, 0.2155],\n",
            "        [0.1746, 0.2038, 0.1998, 0.1988, 0.2230],\n",
            "        [0.2066, 0.1885, 0.1898, 0.1974, 0.2175],\n",
            "        [0.2010, 0.1917, 0.1928, 0.1881, 0.2263],\n",
            "        [0.2188, 0.1832, 0.1926, 0.1783, 0.2271],\n",
            "        [0.2021, 0.1841, 0.2010, 0.1808, 0.2320],\n",
            "        [0.1846, 0.1879, 0.2048, 0.1820, 0.2407],\n",
            "        [0.2277, 0.1799, 0.1872, 0.1822, 0.2231],\n",
            "        [0.1995, 0.1864, 0.1992, 0.1863, 0.2286],\n",
            "        [0.1892, 0.2069, 0.1847, 0.2032, 0.2159],\n",
            "        [0.2095, 0.2129, 0.1888, 0.1826, 0.2062],\n",
            "        [0.2037, 0.2010, 0.1896, 0.1878, 0.2180],\n",
            "        [0.1713, 0.2083, 0.1984, 0.1963, 0.2258],\n",
            "        [0.2086, 0.2078, 0.1782, 0.2005, 0.2049],\n",
            "        [0.1807, 0.2077, 0.1842, 0.2132, 0.2141],\n",
            "        [0.1991, 0.2116, 0.1842, 0.1871, 0.2180],\n",
            "        [0.1856, 0.2219, 0.1857, 0.1903, 0.2165],\n",
            "        [0.2250, 0.1998, 0.1807, 0.1949, 0.1995],\n",
            "        [0.2097, 0.2052, 0.1909, 0.1943, 0.1999],\n",
            "        [0.1774, 0.2249, 0.1905, 0.1930, 0.2142],\n",
            "        [0.2318, 0.1918, 0.1820, 0.1898, 0.2046],\n",
            "        [0.1962, 0.2095, 0.1888, 0.1953, 0.2102],\n",
            "        [0.1855, 0.2251, 0.1848, 0.1918, 0.2128],\n",
            "        [0.1899, 0.2163, 0.1811, 0.2023, 0.2105]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "04/28 10:11:23 PM | epoch  15 |   100/  103 batches | lr 20.00 | ms/batch 3190.78 | loss  6.46 | ppl   638.11\n",
            "04/28 10:12:39 PM -----------------------------------------------------------------------------------------\n",
            "04/28 10:12:39 PM | end of epoch  15 | time: 398.70s | valid loss  6.63 | valid ppl   759.27\n",
            "04/28 10:12:39 PM -----------------------------------------------------------------------------------------\n",
            "04/28 10:12:39 PM Saving Normal!\n",
            "Traceback (most recent call last):\n",
            "  File \"train_search.py\", line 272, in <module>\n",
            "    train()\n",
            "  File \"train_search.py\", line 210, in train\n",
            "    args.unrolled)\n",
            "  File \"/content/drive/MyDrive/B19EE046_Q4/darts/rnn/architect.py\", line 48, in step\n",
            "    hidden = self._backward_step_unrolled(hidden_train, input_train, target_train, hidden_valid, input_valid, target_valid, eta)\n",
            "  File \"/content/drive/MyDrive/B19EE046_Q4/darts/rnn/architect.py\", line 65, in _backward_step_unrolled\n",
            "    unrolled_loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 363, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 175, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd darts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTyvRBl-RWzN",
        "outputId": "1aed7cd2-c616-42a4-dae1-7c31d6c318b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1igaRi_LRURV8nFC-jyoXND54rA2solUZ/B19EE046_Q4/darts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd cnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC92b49ERbAC",
        "outputId": "56b41e0a-6ccc-4d8a-c583-9c32a680758d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1igaRi_LRURV8nFC-jyoXND54rA2solUZ/B19EE046_Q4/darts/cnn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --auxiliary --cutout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ5Kuo_KReR7",
        "outputId": "73de6627-e0a5-4e76-99a7-39960bc3c478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment dir : eval-EXP-20220429-122452\n",
            "04/29 12:24:53 PM gpu device = 0\n",
            "04/29 12:24:53 PM args = Namespace(arch='DARTS', auxiliary=True, auxiliary_weight=0.4, batch_size=96, cutout=True, cutout_length=16, data='../data', drop_path_prob=0.2, epochs=600, gpu=0, grad_clip=5, init_channels=36, layers=20, learning_rate=0.025, model_path='saved_models', momentum=0.9, report_freq=50, save='eval-EXP-20220429-122452', seed=0, weight_decay=0.0003)\n",
            "108 108 36\n",
            "108 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 72\n",
            "144 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 144\n",
            "288 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "04/29 12:24:55 PM param size = 3.349342MB\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:729: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n",
            "04/29 12:24:59 PM epoch 0 lr 2.499966e-02\n",
            "train.py:128: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\n",
            "04/29 12:25:02 PM train 000 3.218843e+00 15.624999 63.541664\n",
            "04/29 12:26:12 PM train 050 3.200466e+00 13.623366 59.211599\n",
            "04/29 12:27:22 PM train 100 3.122592e+00 15.594059 64.088281\n",
            "04/29 12:28:32 PM train 150 3.013472e+00 19.177704 69.239788\n",
            "04/29 12:29:43 PM train 200 2.924387e+00 21.605513 72.864840\n",
            "04/29 12:30:53 PM train 250 2.837349e+00 23.979083 75.684758\n",
            "04/29 12:32:03 PM train 300 2.764682e+00 26.003598 77.720097\n",
            "04/29 12:33:13 PM train 350 2.705904e+00 27.703584 79.246792\n",
            "04/29 12:34:24 PM train 400 2.650536e+00 29.288756 80.668119\n",
            "04/29 12:35:34 PM train 450 2.601943e+00 30.723391 81.804321\n",
            "04/29 12:36:44 PM train 500 2.553149e+00 32.146123 82.749083\n",
            "04/29 12:37:14 PM train_acc 32.747999\n",
            "train.py:150: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  input = Variable(input, volatile=True).cuda()\n",
            "train.py:151: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  target = Variable(target, volatile=True).cuda()\n",
            "04/29 12:37:14 PM valid 000 1.460533e+00 43.750000 90.625000\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 169, in <module>\n",
            "    main() \n",
            "  File \"train.py\", line 105, in main\n",
            "    valid_acc, valid_obj = infer(valid_queue, model, criterion)\n",
            "  File \"train.py\", line 153, in infer\n",
            "    logits, _ = model(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1igaRi_LRURV8nFC-jyoXND54rA2solUZ/B19EE046_Q4/darts/cnn/model.py\", line 150, in forward\n",
            "    s0, s1 = s1, cell(s0, s1, self.drop_path_prob)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1igaRi_LRURV8nFC-jyoXND54rA2solUZ/B19EE046_Q4/darts/cnn/model.py\", line 52, in forward\n",
            "    h2 = op2(h2)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/1igaRi_LRURV8nFC-jyoXND54rA2solUZ/B19EE046_Q4/darts/cnn/operations.py\", line 47, in forward\n",
            "    return self.op(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 141, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 447, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 444, in _conv_forward\n",
            "    self.padding, self.dilation, self.groups)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 11.17 GiB total capacity; 10.23 GiB already allocated; 5.19 MiB free; 10.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8G-piiLSDib",
        "outputId": "6743077d-d81f-4c05-e845-41edcce56fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1igaRi_LRURV8nFC-jyoXND54rA2solUZ/B19EE046_Q4/darts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd rnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9Pnf357SE5_",
        "outputId": "f4ad4d90-7329-4bba-9ce0-fea481a1eeba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1igaRi_LRURV8nFC-jyoXND54rA2solUZ/B19EE046_Q4/darts/rnn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data '/content/drive/.shortcut-targets-by-id/1igaRi_LRURV8nFC-jyoXND54rA2solUZ/B19EE046_Q4/awd-lstm-lm/data/penn'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hTXIkTeSF1u",
        "outputId": "ddf5e29b-a2cf-4573-bb6a-b7945b3f7889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment dir : eval-EXP-20220429-124642\n",
            "torch.Size([14524, 64])\n",
            "torch.Size([7376, 10])\n",
            "torch.Size([82430, 1])\n",
            "04/29 12:46:50 PM Args: Namespace(alpha=0, arch='DARTS', batch_size=64, beta=0.001, bptt=35, clip=0.25, continue_train=False, cuda=True, data='/content/drive/.shortcut-targets-by-id/1igaRi_LRURV8nFC-jyoXND54rA2solUZ/B19EE046_Q4/awd-lstm-lm/data/penn', dropout=0.75, dropoute=0.1, dropouth=0.25, dropouti=0.2, dropoutx=0.75, emsize=850, epochs=8000, gpu=0, log_interval=200, lr=20, max_seq_len_delta=20, nhid=850, nhidlast=850, nonmono=5, save='eval-EXP-20220429-124642', seed=1267, single_gpu=True, small_batch_size=64, wdecay=8e-07)\n",
            "04/29 12:46:50 PM Model total parameters: 22960000\n",
            "04/29 12:46:50 PM Genotype: Genotype(recurrent=[('sigmoid', 0), ('relu', 1), ('relu', 1), ('identity', 1), ('tanh', 2), ('sigmoid', 5), ('tanh', 3), ('relu', 5)], concat=range(1, 9))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "train.py:210: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "04/29 12:46:50 PM rolling back to the previous best model ...\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 252, in <module>\n",
            "    train()\n",
            "  File \"train.py\", line 216, in train\n",
            "    if np.isnan(total_loss[0]):\n",
            "IndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 255, in <module>\n",
            "    model = torch.load(os.path.join(args.save, 'model.pt'))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 699, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 231, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 212, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'eval-EXP-20220429-124642/model.pt'\n"
          ]
        }
      ]
    }
  ]
}